\documentclass[a4paper]{article}

\input{temp}

\begin{document}

\title{Principle of Statistics (continued) }

\maketitle

\newpage

\tableofcontents

\newpage

In the previous lecture, we saw that for a bootstrap sample $(X_1^b,...,X_n^b)$ drawn from $\P_n( \mid X_1,...,X_n)$, we have $\sup_{t \in \R} | \P_n(\sqrt{n} (\bar{X}_n^b - \bar{X}_n) \leq t(X_1,...,X_n) - \Phi(t) | \xrightarrow{a.s.} 0$ as $n \to \infty$, where $\Phi(t) = \P(Z \leq t)$, $Z \sim N(0,\sigma^2)$.

\begin{rem}
As in the B.vM theorem, this theorem can be used to show that $\P(\mu \in \mathcal{C}_n) \to 1-\alpha$ when $n \to \infty$.
\end{rem}

Idea: For fiexd $(X_i)_{i \geq 1}$, $\P_n(\mid X_1,...,X_N)$ is a sequence of distributions. Considering the $X_i$ as independent random variables drawn from $P$ allows us to make statements "with randomness". The idea is to fix a sequence $X_i$ (equivalent to fix $\omega$ in the original probability space) if we can show that $\sup_{t \in \R} |\P_n(...\mid X_1,...,X_n) - F(t)| \to 0$ "for almost all $\omega"$, then we have almost sure convergence.

\begin{lemma}
If $A_n \sim f_n \xrightarrow{d} A \sim f$, and $F$ is continuous (c.d.f of $f$), then $\sup|F_n(t) - F(t)| \to 0$ as $n \to \infty$.
\begin{proof}
By continuity of $F$, there exists points $-\alpha_0 = x_0 < x_1 < ... < x_k = +\infty$ such that $F(x_i) = \frac{i}{k}$. Then for every $x \in [x_{i-1},x_i]$, $F_n(x) - F(x) \leq F_n(x_i) - F(x_{i-1}) = F_n(xi)-F(x_i) + \frac{1}{k}$, and $F_n(x) - F(x) \geq F_n(x_{i-1}) - F(x_i) = F_n(x_{i-1}) - F(x_{i-1}) - \frac{1}{k}$. For $k$ large enough, $\frac{1}{k} < \frac{\varepsilon}{2}$, for $n$ large enough (dependes on $k$), we have $\max_{0 \leq i \leq k} |F_n(x_i) - F(x_i)| < \varepsilon/2$ (pointwise convergence) (dependes on $k$), we have $$\max_{0 \leq i \leq k} |F_n(x_i) - F(x_i)| < \varepsilon/2$$ (pointwise convergence). As a consequence, $$\sup_{x \in \R} |F_n(x) - F(x)| \leq \max_{0 \leq i \leq k} |F_n(x_i) - F(x_i)| + \frac{1}{k}<\varepsilon$$
\end{proof}
\end{lemma}

\begin{defi}
The sequence $(Z_{n,i},i=1,...,n)_{n \geq 1}$ is a triangular array of i.i.d. random variables if:\\
$\bullet$ For all $n \geq 1$, $(Z_{n,1},...,Z_{n,i},...,Z_{n,n})$ is a sequence of i.i.d. random variables;\\
For example, $Z_{11} = (Z_{1,i})$,\\
$Z_{21},Z_{22} = (Z_{2,i})$,\\
...,\\
$Z_{n,1},...,Z_{n,n} = (Z_{n,i})$.\\
We need independence on each line, but not across the lines. We don't need even need the same distribution at each line.
\end{defi}

\begin{prop} (CLT for triangular arrays)\\
Let $(Z_{n,i};i=1,...,n)$ be a triangular array of iid random variables, each with finite variance. We have $Var_{Q_n}(Z_{n,i}) = \sigma_n^2 \to \sigma_2$ as $n \to \infty$, each line consists of $n$ independent draws from $Q_n$. Then, under the following hypotheses (1-3), we have $$\sqrt{n} (\frac{1}{n} \sum_{i=1}^n Z_{n,i} - \E_{Q_n} [Z_{n,i}])\xrightarrow{d} N(0,\sigma^2)$$
(1) $\forall \delta>0$, $n Q_n (|Z_{n,1}|>\sqrt{n}\delta) \to 0$ as $n \to \infty$;\\
(2) $Var(Z_{n,1} 1\{|Z_{n,1}|\leq \sqrt{n}\}) \to \sigma^2$ as $n \to \infty$;\\
(3) $\sqrt{n} \E [Z_{n,1} 1\{|Z_{n,1}| > \sqrt{n}\}] \to 0$ as $n \to \infty$.\\
(The statement of these assumptions is not examinable.)
\begin{proof} (of the main theorem)\\
Fix $(X_i)_{i \geq 1}$ (equivalent to fix $\omega$ in the original probability space). Under $Q_n = \P_n(\cdot \mid X_1,...,X_n)$, $Z_{n,i} = X_i^{b(n)}$ (bootstrap on $n$ observations), $\E_n[Z_{n,i}] = \E_{\P_n} [X_i^{b(n)}] = \bar{X}_n$. Then, the $(X_i^{b(n)};i=1,...,n)_{n \geq 1}$ are a triangular array of i.i.d. variables. We have that $$Var_{\P_n}(X_i^{b(n)}) = \E_{\P_n} [X_i^{b(n)^2}] - (\E_{\P_n} [X_i^{b(n)}])^2 = \frac{1}{n}\sum_{i=1}^n X_i^2-(\frac{1}{n}\sum_{i=1}^n X_i)^2 = \sigma_n^2$$ by definition of $\P_n(\mid X_1,...,X_n)$. For almost all the $\omega$, or almost all infinite sequences, $\sigma_n^2 \to \sigma^2$, and hypotheses (1-3).\\
$\bullet$ By the CLT for triangular arrays, we have that $$\sqrt{n} (\bar{X}_n^{b(n)} - \bar{X}_n) \xrightarrow{d} N(0,\sigma^2)$$ as $n \to \infty$ 'for almost all $\omega$'.\\
$\bullet$ By lemma, $$\sup_{t \in \R} | \P_n (\sqrt{n} (\bar{X}_n^{b(n)} - \bar{X}_n) \leq t) - \Phi(t)| \geq 0$$ as $n \to \infty$ 'for almost all $\omega$', meaning that it $\xrightarrow{a.s.} 0$.
\end{proof}
\end{prop}

\begin{rem}
$\bullet$ This shows the validity of the bootstrap confidence interval for the mean.\\
$\bullet$ In genral, this can be extended to estimation of $\theta$: Sampling from $\P_n$ as for the mean, and compute the bootstrap MLE $\hat{\theta}_n^b = \hat{\theta} (X_1^b,...,X_n^b)$ then using $\sqrt{n}(\hat{\theta}_n^b - \hat{\theta}_n)$ as a proxy for $\sqrt{n} (\hat{\theta}_n - \theta_0)$, we will have similar results that show that taking $R_n$ such that $$\P_n(||\hat{\theta}_n^b - \hat{\theta}_n|| \leq \frac{R_n}{\sqrt{n}} \mid X_1,...,X_n) = 1-\alpha$$ can be used to construct a valid confidence region.\\
This approach is known as the non-parametric bootstrap.\\
Another approach is to do the same thing with $X_1^b,...,X_n^b \sim \P_{\hat{\theta}_n}$, same types of results will hold.
\end{rem}

Last lecutre on Monday. \emph{No} Lecture on Wednesday!

\subsection{Monte-Carlo methods}
In statistics, we often cannot explicityl compute expectation/integrals, which can be problematic when:\\
$\bullet$ We want to obtain a posterior distribution;\\
$\bullet$ We want to obtain a posterior mean;\\
$\bullet$ We want to compute quantiles of a distribution.

One of the ideas of Monte-Carlo methods is to replace explicitly computation by \emph{simulations}Last lecutre on Monday. \emph{No} Lecture on Wednesday!

\subsection{Monte-Carlo methods}
In statistics, we often cannot explicityl compute expectation/integrals, which can be problematic when:\\
$\bullet$ We want to obtain a posterior distribution;\\
$\bullet$ We want to obtain a posterior mean;\\
$\bullet$ We want to compute quantiles of a distribution.

One of the ideas of Monte-Carlo methods is to replace explicitly computation by \emph{simulations}. One of the first challenges is to draw from a given, general distribution.

\begin{defi}
A pseudorandom generator provides independent $U_i^{*} \sim Unif(0,1)$.
\end{defi}

\begin{rem}
They are generated such that for all practical uses, $P(U_1^* \leq u_1,...,U_N^* \leq u_n) = \prod_{i=1}^N u_i$ (up to 'machine precision'). Here it can be thought of as a blackbox that outputs i.i.d. uniform numbers -- this can be used as a starting point to generate other variables.
\end{rem}

\begin{prop}
The random variables $K_i = \sum_{k=1}^n k 1_{(\frac{k-1}{n},\frac{k}{n}]} (U_i^*)$ are i.i.d. uniform on $\{1,...,n\}$.
\begin{proof}
$K_i$ is clearly uniform on $\{1,...,n\}$, as each segment has length $\frac{1}{n}$. They are independent as functions of independent random variables $U_i^*$. 
\end{proof}
\end{prop}
\begin{rem}
Assigining other values to each of the intervals with uniform distribution on any set of size $n$. In particular, we can simulate bootstrap samples by writing $$X_i^b = \sum_{k=1}^n X_k 1_{(\frac{k-1}{n},\frac{k}{n}]} (U_i^*)$$ If the intervals are chosen with different lengths, we can generate any discrete distribution. For a general distribution with cdf $F$, we can generalize this idea.
\end{rem}

\begin{defi}
For a general cdf $F$, we define the generalized inverse of $F$ as $$F^-(u) = \inf \{x: u \leq F(x)\}$$
\end{defi}

\begin{rem}
For a fixed value $t \in \R$, the function $F$ gives $F(t) \in [0,1]$ a probability $\P(X \leq t)$. For a fixed value $u \in [0,1]$, the function $F$ gives $t = F^- (u)$ such that approximately $\P(X \leq t) = u$.
\end{rem}

\begin{prop}
$X = F^-(u)$ for $U \sim Unif (0,1)$ has a distribution cdf $F$.
\begin{proof} (Example sheet)\\
$\P(X \leq t) = \P(F^-(U) \leq t) = ... = F(t)$, and use that $\P(U \leq z) = z$ for $z \in (0,10$.
\end{proof}
\end{prop}
Conclusion (of the first part): If $F$ is known, explicit, we can generated $(X_1^*,...,X_N^*) = (F^-(U_1^*),...,F^-(U_N^*))$ that are i.i.d., each with cdf $F$. If we want to compute $\E_{X \sim f} [g(X)]$, we can approximate it by $\frac{1}{N} \sum_{i=1}^N g(X_i^*)$, and use the fact that $\frac{1}{N} \sum_{i=1}^N g(X_i^*) \xrightarrow{a.s.} \E[g(X)]$ by the LLN.

In certain situations, the distributino might be complex, and $F$, $F^-$ are not explicit. For example, $N(\mu,\sigma^2)$ can be solved by looking up in a table, but $\Pi (\cdot \mid X)$ can involve complicated integrals (the density) which make computation of $F_\Pi$ impossible.

There are several ways to tackle this and approximately sample from distributions.

(1) Importance sampling: Let $F$ have density $f$, and random variables i.i.d. $X_i^* \sim h$.

\begin{prop}
$\E_h \left[\frac{g(x)}{h(x)} f(x)\right] = \E_f [g(x)]$.
\begin{proof}
The above is equal to $$ \int_\chi \frac{g(x)}{h(x)} f(x) \cdot h(x) dx = \int_\chi g(x) f(x) dx$$ As a consequence, $$\frac{1}{N} \sum_{i=1}^n \frac{g(X_i^*)}{h(X_i^*)} f(X_i^*) \xrightarrow{a.s.} \E_{X \sim f} [g(X)]$$.
\end{proof}
\end{prop}

(2) Accept/Reject algorithm. As in (1), but $f \leq M\cdot h$ for some constant $M$.\\
Step 1: generate $X \sim h$ and $U \sim U(0,1)$.\\
Step 2: $Y=X$ if $U \leq \frac{f(X)}{M\cdot h(X)}$, otherwise return to step 1. Then $Y \sim f$ (example sheet).

For multivariate problems where conditional distributions are aesy to compute but not joint distributions, we can then use the \emph{Gribbs samples}: In the bivariate case $(X,Y)$, start at the same $X = x_0$, and $Y_1 \sim f_{Y|X}(\cdot |x_0)$, and $X_1 \sim f_{X|Y}(\cdot |y_1)$, etc., $Y_t \sim f_{Y|X}(\cdot | X_{t-1})$, $X_t \sim f_{X|Y}(\cdot | Y_t)$. The sequences $(X_t,Y_t)$,$(X_t)$,$(Y_t)$ are all Markov chains, with invariant distribution $f$, $f_{X|Y}$, $f_{Y|X}$. And we can use the ergodic theorem to approximate expectations $$\frac{1}{N} g(X_t,Y_t) \to \E_{(X,Y) \sim f} [g(X,Y)]$$ This can be used in particular in the case $Q(x,\theta) = f(x,\theta) \pi (\theta)$.

\end{document}
