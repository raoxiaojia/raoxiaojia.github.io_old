\documentclass[a4paper]{article}

\input{temp}

\setcounter{section}{-1}

\begin{document}

\title{Analytic Number Theory}

\maketitle

\newpage

\tableofcontents

\newpage

\section{Introduction}

---Lecture 1---

Lecturer: Thomas Bloom ($tb634@cam.ac.uk$, $www.thomasbloom.org/ant.html$)

Printed notes will be updated, but 1-2 weeks behind.

Example classes: weeks 3,5,7, tuesdays 330-5pm; prop-in sessions weeks 2,4,6,8. Rooms to be confirmed later.

What is analytic number theory? It's the study of numbers (regular integers, discrete) using analysis (real/complex, continuous) and some other quantitative questions.

For example, for the famous function $\pi(x)$, the number of primes no greater than $x$, we know $\pi(x) \sim \frac{x}{\log x}$.

Throughout this course, by \emph{numbers} we'll mean natural numbers excluding 0.

We can also ask how many twin primes there are, i.e. how many $p$ such that $p,p+2$ are both prime. This is not known yet (not even the finiteness); but from 2014, Zhang, Maynard, Polymath showed that there are infinitely many primes at most 246 apart, which is not that far from 2. The current guess is that the number is around $\frac{x}{(\log x)^2}$.

Another question we may ask: how many primes are there $\equiv a \pmod q$, $(a,q) = 1$. We know by Dirichlet's theorem that there are infinitely many.\\
A natural guess of the count is $\frac{1}{\phi(q)} \frac{x}{\log x}$, where $\phi(x)$ is the Euler Totient function. This is known to hold for small $q$.

In this course we'll talk about:\\
(1) Elementary techniques (real analysis);\\
(2) Sieve methods;\\
(3) Riemann zeta function/prime number theory (complex analysis);\\
(4) Primes in arithmetic progressions.

\newpage

\section{Elementary techniques}
Review of asymptotic notation:\\
$\bullet$ $f(x) = O(g(x))$ if there is $c>0$ s.t. $|f(x)| \leq c|g(x)|$ for all large enough $x$;\\
$\bullet$ $f \ll g$ is the same thing as $f=O(g)$. This also defines what $f \gg g$ means in the natural way;\\
$\bullet$ $f \sim g$ if $\lim_{x \to \infty} \frac{f(x)}{g(x)} = 1$ (i.e. $f=(1+o(1))g$);\\
$\bullet$ $f=o(g)$ if $\lim_{x \to \infty} \frac{f(x)}{g(x)} = 0$.

\subsection{Arithmetic functions}
Arithmetic functions are just functions $f:\N \to \C$; in other words, relabelling natural numbers with some complex numbers.\\
An important operation for multiplicative number theory ($fg = f(n)g(n)$) is multiplicative convolution,
$$f*g(n) = \sum_{ab=n}f(a)g(b)$$

Examples: $1(n) \equiv 1 \forall n$ (caution: $1$ is not the identity function, and $1*f \neq f$).\\
M\"{o}bius function:
\begin{equation*}
\begin{aligned}
\mu(n) = \left\{\begin{array}{ll}
(-1)^k & \text{ if } n=p_1...p_k\\
0 & \text{ if } n \text{ is divisible by a square} 
\end{array}
\right.
\end{aligned}
\end{equation*}
Liouville function: $\lambda(n) = (-1)^k$ if $n=p_1...p_k$ (primes not necessarily distinct),\\
Divisor function: $\tau(n) = $ number of $d$ s.t. $d|n = \sum_{ab = n} 1 = 1*1$. This is sometimes also known as $d(n)$.

An arithmetic function is multiplicative if $f(nm) = f(n)f(m)$ when $(n,m)=1$.\\
In particular, a multiplicative function is determined by its values on prime powers.

\begin{fact}
If $f,g$ are multiplicative, then so is $f*g$.\\
All the function we've seen so far ($\mu,\lambda,\tau,1$) are multiplicative.
\end{fact}

Non-example: $\log n$ is definitely not multiplicative.

\begin{fact} (M\"{o}bius inversion)\\
$1*f=g \iff \mu*g=f$. That is,
$$\sum_{a|n} f(d) = g(n) \forall n \iff \sum_{d|n} g(d)\mu(n/d) = f(n) \forall n$$
e.g.
\begin{equation*}
\begin{aligned}
\sum_{d|n}\mu(d) = \left\{\begin{array}{ll}
1 & n=1\\
0 & \text{else}
\end{array}
\right. = 1*\mu
\end{aligned}
\end{equation*}
is multiplicative: it's enough to check identity for primes powers.\\
If $n=p^k$ then $\{d|n\} = \{1,p,...,p^k\}$. So LHS=$1-1+0+0+...=0$, unless $k=0$ when LHS = $\mu(1) = 1$.

Our goal is to study primes. The first guess might be to work with
\begin{equation*}
\begin{aligned}
1_p(n) = \left\{\begin{array}{ll}
1 & n \text{ prime}\\
0 & \text{else}
\end{array}
\right.
\end{aligned}
\end{equation*}
(e.g. $\pi(x) = \sum_{1 \leq n \leq x} 1_p(n)$). Instead, we work with von Mangoldt funcion
\begin{equation*}
\begin{aligned}
\wedge(n) = \left\{\begin{array}{ll}
\log p & n \text{ is a prime power}\\
0 & \text{else}
\end{array}
\right.
\end{aligned}
\end{equation*}
(e.g. in a few lectures we'll look at $\psi(x) = \sum_{1\leq n \leq x} \wedge(n)$).
\end{fact}

\begin{lemma} (1)\\
$1*\wedge = \log$, and by M\"{o}bius inversion, $\mu*\log = \wedge$.\\
Note that it's easy to realize that $\wedge$ is not multiplicative, else $\log$ will be.
\begin{proof}
$1*\wedge(n) = \sum_{d | n} \wedge(d)$. So if $n=p_1^{k_1}...p_r^{k_r}$, then above
\begin{equation*}
\begin{aligned}
&=\sum_{i=1}^r \sum_{j=1}^{k_i} \wedge(p_i^j)\\
&=\sum_{i=1}^r \sum_{j=1}^{k_i} \log(p_i)\\
&= \sum_{i=1}^r k_i \log (p_i)\\
&= \log n
\end{aligned}
\end{equation*}
\end{proof}
\end{lemma}

Note that the above tells us
\begin{equation*}
\begin{aligned}
\wedge(n) &= \sum_{d|n} \mu(d) \log(n/d)\\
&=\log n \sum_{d|n}\mu(d) - \sum_{d|n} \mu(d)\log d\\
&= -\sum_{d|n} \mu(d)\log d
\end{aligned}
\end{equation*}
by the famous fact that $\sum_{d|n}\mu(d)=0$ unless $n=1$; but when $n=1$, $\log n = 0$.\\
Now we can try to evaluate
\begin{equation*}
\begin{aligned}
-\sum_{1 \leq n \leq x} \wedge(n) &= \sum_{1 \leq n \leq x} \sum_{d|n} \mu(d) \log d \\
&= -\sum_{d \leq x} \mu(d) \log d (\sum_{1 \leq n \leq x, d|n} 1)\ \text{ (reverse order of summation)}
\end{aligned}
\end{equation*}
But
$$\sum_{1 \leq n \leq x, d|n} 1 = \lfloor x/d \rfloor = x/d + O(1)$$
So we know the original sum is equal to
$$-x\sum_{d \leq x} \mu(d) \frac{\log d}{d} + O(\sum_{d \leq x} \mu(d) \log d)$$

---Lecutre 2---

Lecturer's favourite book: \emph{Multiplicative Number Theory}.

Room for example classes: MR14 (Tues 330-5pm, week 357).

\subsection{Summation}
Given an arithmetic function $f$, we can ask for estimates of $\sum_{1 \leq n \leq x} f(n)$.\\
We say that $f$ has \emph{average order $g$} if $\sum_{1 \leq n \leq x} f(n) \sim xg(x)$ (in some sense, the average size of $f$ is $g$).

For example, if $f \equiv 1$, then $\sum_{1 \leq n \leq x} f(n) = \lfloor x \rfloor = x+O(1) \sim x$. So the average order of $1$ is $1$ (makes a lot of sense).\\
A slightly less trivial example is the identity function $f(n) = n$: we have $\sum_{1 \leq n \leq x} n \sim \frac{x^2}{2}$, so the average order of $n$ is $n/2$.

\begin{lemma} (1, Partial summation)\\
If $(a_n)$ is a sequence of complex numbers, and $f$ is s.t. $f'$ is continuous. Then $\sum_{1 \leq n \leq x} a_n f(n) = A(x) f(x) - \int_1^x A(t) f'(t) dt$, where $A(x) = \sum_{1 \leq n \leq x} a_n$.\\
We can see that this is a discrete version of integration by parts.
\begin{proof}
Suppose $x=N$ is an integer. Note that $a_n = A(n) - A(n-1)$. So
\begin{equation*}
\begin{aligned}
\sum_{1 \leq n \leq N} a_nf(n) &= \sum_{1 \leq n \leq N} f(n) (A(n)-A(n-1))\\
&= A(N)f(N) - \sum_{n=1}^{N-1}A(n) (f(n+1)-f(n)) \text{ using } A(0)=0
\end{aligned}
\end{equation*}
Now $f(n+1)-f(n) = \int_n^{n+1} f'(t)dt$. So
\begin{equation*}
\begin{aligned}
\sum_{1 \leq n \leq N} a_n f(n) &= A(N)f(N) - \sum_{n=1}^{N-1} A(n) \int_n^{n+1} f'(t) dt\\
&= A(N)f(N) - \int_1^N A(t) f'(t) dt
\end{aligned}
\end{equation*}
To be complete, we should also consider the case where $x$ is not an integer. But if $N=\lfloor x \rfloor$,
\begin{equation*}
\begin{aligned}
A(x) f(x) &= A(N)f(x)\\
&=A(N)\left(f(N)+\int_N^x f'(t)dt\right)
\end{aligned}
\end{equation*}
\end{proof}
\end{lemma}

\begin{lemma} (2)\\
$$\sum_{1 \leq n \leq x} \frac{1}{n} = \log x + \gamma + O\left(\frac{1}{x}\right)$$
where $\gamma$ is some constant.
\begin{proof}
Apply partial summation with $f(x)=\frac{1}{x}$ and $a_n \equiv 1$, so $A(x) = \lfloor x \rfloor$. Then, writing $\lfloor t \rfloor = t - \{t\}$,
\begin{equation*}
\begin{aligned}
\sum_{1 \leq n \leq x} \frac{1}{n} &= \frac{\lfloor x \rfloor}{x} + \int_1^x \frac{\lfloor t \rfloor}{t^2} dt\\
&= 1 + O\left(\frac{1}{x}\right) + \int_1^x \frac{1}{t} dt - \int_1^x \frac{\{t\}}{t^2} dt\\
&= 1+O\left(\frac{1}{x}\right) + \log x - \int_1^\infty \frac{\{t\}}{t^2} dt + \int_x^\infty \frac{\{t\}}{t^2} dt\\
&=\gamma + O\left(\frac{1}{x}\right) + \log x + O\left(\frac{1}{x}\right)\\
&= \log x + \gamma + O\left(\frac{1}{x}\right)
\end{aligned}
\end{equation*}
where at the penultimate step we bound the error term by 
\begin{equation*}
\begin{aligned}
\int_x^\infty \frac{\{t\}}{t^2} dt &\leq \int_x^\infty \frac{1}{t^2} dt\\
&\leq \frac{1}{x}
\end{aligned}
\end{equation*}
and we actually know $\gamma = 1 - \int_1^\infty \frac{\{t\}}{t^2} dt$.\\
This $\gamma$ is called Euler's constant (Euler-Mascheroni).\\
We know very little about this constant: we only know $\gamma=0.577...$, and we don't even know if $\gamma$ is irrational.
\end{proof}
\end{lemma}

\begin{lemma} (3)\\
$$\sum_{1 \leq n \leq x} \log n = x\log x - x + O(\log x)$$
\begin{proof}
Use partial summation again, with $f(x) = \log x$ and $a_n=1$, so $A(x) = \lfloor x \rfloor$:
\begin{equation*}
\begin{aligned}
\sum_{1 \leq n \leq x} \log n &= \lfloor x \rfloor \log x - \int_1^x \frac{\lfloor t \rfloor}{t} dt\\
&= x \log x + O(\log x) - \int_1^x 1 dt + O(\int_1^x \frac{1}{t} dt)\\
&= x\log x - x + O(\log x)
\end{aligned}
\end{equation*}
\end{proof}
\end{lemma}

\subsection{Dinsar function}
Recall that $\tau(n) = 1*1(n) =\sum_{d|n} 1$.

\begin{thm} (4)\\
$$\sum_{1 \leq n \leq x} \tau(n) = x\log x + (2\gamma - 1) x + O(x^{1/2})$$
So average order of $\tau$ is $\log x$.
\begin{proof}
Note that we won't apply partial summation here: PS allows to get $\sum a_n f(n)$ from knowledge of $\sum a_n$; but $\tau(n)$ here is not differentiable, so PS is not going to apply.
\begin{equation*}
\begin{aligned}
\sum_{1 \leq n \leq x} \tau(n) &= \sum_{1 \leq n \leq x} \sum_{d|n} 1\\
&= \sum_{1 \leq d \leq x} \sum_{1 \leq n \leq x, d|n} 1\\
&= \sum_{1 \leq d \leq x} \lfloor \frac{x}{d} \rfloor\\
&= \sum_{1 \leq d \leq x} \frac{x}{d} + O(x)\\
&= x \sum_{1 \leq d \leq x} \frac{1}{d} + O(x)\\
&= x\log x + \gamma x + O(x) 
\end{aligned}
\end{equation*}
where we applied lemma 2 at the last step. This is all correct, but the error term is larger than what we wanted. However, we have indeed prove that the average order of $\tau(x)$ is $\log x$.\\
To reduce error term, we use (Dirichlet's) hyperbola trick:
\begin{equation*}
\begin{aligned}
\sum_{1 \leq n \leq x} \tau(n) &= \sum_{1 \leq n \leq x} \sum_{ab = n} 1\\
&= \sum_{ab \leq x} 1\\
&= \sum_{a \leq x} \sum_{b \leq \frac{x}{a}} 1
\end{aligned}
\end{equation*}
Note that now we're just counting number of integer points below the hyperbola $xy=n$ (relabelling variables).\\
When summing over $ab \leq x$, we can sum over $a,b \leq x^{1/2}$ separately, then subtract the repetition off. Then
\begin{equation*}
\begin{aligned}
\sum_{1 \leq n \leq x} \tau (n) &= \sum_{a \leq x^{1/2}}\sum_{b \leq \frac{x}{a}} 1 + \sum_{b \leq x^{1/2}} \sum_{a \leq \frac{x}{b}} 1 - \sum_{a,b \leq x^{1/2}} 1\\
&= 2 \sum_{a \leq x^{1/2}} \lfloor \frac{x}{a} \rfloor - \lfloor x^{1/2} \rfloor^2\\
&= 2 \sum_{a \leq x^{1/2}} \frac{x}{a} + O(x^{1/2}) - x + O(x^{1/2})
\end{aligned}
\end{equation*}
by noting that $\lfloor x^{1/2}\rfloor^2 = (x^{1/2} + O(1))^2$. Now the above equals
\begin{equation*}
\begin{aligned}
&=2x\log x^{1/2} + 2\gamma x - x + O(x^{1/2})\\
&= x\log x + (2\gamma-1) x + O(x^{1/2})
\end{aligned}
\end{equation*}
\end{proof}

Improving this $O(x^{1/2})$ error term is a famous and hard problem. We should probably get $O(x^{1/4+\varepsilon})$, but this is open. The best known result is $O(x^{0.3149...})$.
\end{thm}

Note that this does \emph{not} mean that $\tau(n) \ll \log n$. The average order is small doesn't say about the individual values being small.

We'll state the theorem we're proving and prove it in the next lecture:
\begin{thm} (5)\\
$$\tau(n) \leq n^{O(\frac{1}{\log\log n})}$$
In particular, $\tau(n) \ll_\varepsilon n^\varepsilon$ $\forall \varepsilon > 0$.

---Lecture 3---
\begin{proof}
    $\tau$ is multiplicative, so it's enough to calculate at prime powers.\\
    Now, $\tau(p^k) = k+1$. So if $n=p_1^{k_1}...p_r^{k_r}$, thjen $\tau(n) = \prod_{i=1}^r (k_i+1)$.\\
    Let $\varepsilon$ be chosen later, and consider $\frac{\tau(n)}{n^\varepsilon} = \prod_{i=1}^r \frac{k_i+1}{p_i^{k_i\varepsilon}}$. Note as $p \to \infty$, $\frac{k+1}{p^{k\varepsilon}} \to 0$.\\
    In particular, if $p \geq 2^{1/\varepsilon}$, then $\frac{k+1}{p^{k\varepsilon}} \leq \frac{k+1}{2^k} \leq 1$. What about for small $p$? We can't do better than $p \geq 2$, but that's enough.\\
    In this case, $\frac{k+1}{p^{k\varepsilon}} \leq \frac{k+1}{2^{k\varepsilon}} \leq \frac{1}{\varepsilon}$ (as $x+\frac{1}{2} \leq 2^x \implies \varepsilon k + \varepsilon \leq 2^{k\varepsilon} \forall x \geq 0$), for $\varepsilon \leq 1/2$.\\
    So
    \[
        \frac{\tau(n)}{n^\varepsilon} \leq \prod_{i=1,_i < 2^{1/\varepsilon}}^r \frac{k_i+1}{p^{k_i \varepsilon}} \leq (1/\varepsilon)2^{1/\varepsilon}
    \]
    Now choose optimal $\varepsilon$:\\
    (trick!) if you want to choose $x$ to minimise $f(x)+g(x)$, choose $x$ s.t. $f(x) = g(x)$.\\
    So here, $\tau(n) \leq n^\varepsilon \varepsilon^{-2^{1/\varepsilon}} = exp(\varepsilon \log n + 2^{1/\varepsilon} \log 1/\varepsilon)$.\\
    Choose $\varepsilon$ s.t. $\log n \approx 2^{1/\varepsilon}$, i.e. $\varepsilon = \frac{1}{\log\log n}$. So
    \[
        \tau(n) &\leq n^{1/\log\log n} (\log\log n)^{2^{\log\log n}}\\
        &= n^{1/\log\log n} e^{(\log n)^{\log 2} \log \log \log n}\\
        &\leq n^{O(\frac{1}{\log \log n})} 
    \]
\end{proof}
\end{thm}

\subsection{Estimates for the Primes}

Recall $\pi(x)$ is the number of primes $\leq x = \sum_{1 \leq n \leq x} 1_p(n)$, and $\psi(x) = \sum_{1 \leq n \leq x} \Lambda(n)$. The prime number theorem states that $\pi(x) \sim \frac{x}{\log x}$, or equivalently $\psi(x) \sim x$ (justified later).\\
It was 1850 before the correct magnitude of $\pi(x)$ was proved. Chebyshev showed that $\pi(x) \asymp x/\log x$, where $f \asymp g$ means $g \ll f \ll g$.

\begin{thm} (6, Chebyshev)\\
    $\psi(x) \asymp x$.\\
    We'll show below that $(\log 2) x \leq \psi(x) \leq (\log 4) x$ (remember that the default base for $\log$ is $e$, so $\log 2 < 1$ while $\log 4 > 1$).
    \begin{proof}
        First we'll prove the lower bound. Recall $1 *\Lambda = \log$, i.e. $\sum_{ab = n} \Lambda(a) = \log n$. The (genuine) trick is to find a sum $\Sigma$ s.t. $\varepsilon \leq 1$(?). We'll use the identity $\lfloor x \rfloor \leq 2 \lfloor \frac{x}{2} \rfloor + 1 \forall x \geq 0$. Why? Say $\frac{x}{2} = n+\theta$, $\theta \in [0,1)$ Then $\lfloor \frac{x}{2} \rfloor = n$, and $x = 2n+2\theta$, and so $\lfloor x \rfloor = 2n$, or at most $2n+1$.\\
        So
        \[
            \psi(x) &\geq \sum_{n \leq x} \Lambda(n) (\lfloor \frac{x}{n} \rfloor - 2\lfloor \frac{x}{2n} \rfloor)\\
            &= \sum_{n \leq x} \Lambda(n) \sum_{m \leq x/n} 1 - 2 \sum_{n \leq x} \Lambda(n) \sum_{m \leq \frac{x}{2n}} 1\\
            &= \sum_{nm \leq x} \Lambda(n) - 2\sum_{nm \leq x/2} \Lambda(n), \text{ write } d = nm,\\
            &= \sum_{d \leq x} 1 *\Lambda(d) - 2 \sum_{d \leq x/2} 1 * \Lambda(d)\\
            &= \sum_{d \leq x} \log d - 2 \sum_{d \leq x/2} \log d\\
            &=x\log x - x + O(\log x) - 2(\frac{x}{2} \log \frac{x}{2} - \frac{x}{2} + O(\log x))\\
            &= (\log 2) x + O(\log x) \gg x
        \]
        For the upper bound, note that $\lfloor x \rfloor = 2\lfloor x/2 \rfloor + 1$ for $x \in (1,2)$, so
        $$\sum_{x/2 < n < x} \Lambda(n) = \sum_{x/2 < n < x} \Lambda(n) (\lfloor x/n \rfloor - 2 \lfloor x/2n \rfloor) \leq \sum_{1 \leq n \leq x} \Lambda(n) (\lfloor x/n \rfloor - 2\lfloor x/2n \rfloor)$$
        so $\psi(x) - \psi(x/2) \leq (\log 2) x + O(\log x)$.\\
        So $\psi(x) = (\psi(x) - \psi(x/2)) + (\psi(x/2)-\psi(x/4))+... \leq \log 2 (x+x/2+x/4+...) = (2\log 2) x$ (note only $\log x$ error terms at most).
    \end{proof}
\end{thm}

\begin{lemma} (7)\\
    \[
        \sum_{p \leq x, p \text{ primes}} \frac{\log p}{p} = \log x + O(1)
    \]
    \begin{proof}
        Recall that $\log = 1 * \Lambda$. So
        \[
            \sum_{n \leq x} \log n &= \sum_{ab \leq x} \Lambda(a)\\
            &= \sum_{a \leq x} \Lambda(a) \sum_{b \leq x/a} 1\\
            &= \sum_{a \leq x} \Lambda(a) \lfloor x/a \rfloor \\
            &= x \sum_{a \leq x} \frac{\Lambda(a)}{a} + O(\psi(x))\\
            &= x \sum_{a \leq x} \frac{\Lambda(a)}{a} + O(x)
        \]
        But $\sum_{n \leq x} \log n = x\log x - x + O(\log x)$. So
        \[
            \sum_{n \leq x} \frac{\Lambda(n)}{n} = \log x - 1 + O(\frac{\log x}{x}) + O(1) + \log x + O(1)
        \]
        It remains to note that 
        \[
            \sum_{p \leq x}\sum_{n=2}^\infty \frac{\log p}{p^n} &= \sum_{p \leq x} \log p \sum_{k=2}^\infty \frac{1}{p^k} \\
            &= \sum_{p \leq x} \frac{\log p}{p^2-p}\\
            &\leq \sum_{p=2}^\infty \frac{1}{p^{3/2}} = O(1)
        \]
        So $\sum_{n \leq x} \frac{\Lambda(n)}{n} = \sum_{p \leq x} \frac{\log p}{p}+O(1)$.
    \end{proof}
\end{lemma}

---Lecture 4---

Drop-in: Tuesday 4pm-5pm.

\begin{lemma} (8)\\
$\pi(x) = \frac{\psi(x)}{\log x} + O(\frac{x}{(\log x)^2})$.\\
In particular, $\pi(x) \asymp \frac{x}{\log x}$ and prime number theorem: $\pi(x) \sim \frac{x}{\log x}$ is equivalent to $\psi(x) \sim x$.\\
So from now on, we'll call this the prime number theorem instead.
\begin{proof}
The idea is to use partial summation:
\[
\theta(x) := \sum_{p \leq x} \log p &= \pi(x) \log x - \int_1^x \frac{\pi(t)}{t} dt
\]
but this doesn't work immediately, since $\psi(x) = \sum_{n \leq x} \Lambda(n) = \sum_{p^k \leq x} \log p$. However, we have
\[
\psi(x)-\theta(x) &= \sum_{k=2}^\infty \sum_{p^k \leq x} \log p\\
&= \sum_{k=2}^\infty \theta(x^{1/k})\\
&\leq \sum_{k=2}^\infty \psi(x^{1/k})\\
&= \sum_{k=2}^{\log x} \psi(x^{1/k})
\]
as the larger terms are all zero. Then the above
\[
\ll \sum_{k=2}^{\log x} x^{1/k}\\
\ll x^{1/2} \log x
\]
(Obviously we could do better, but that's less important). Now $\psi(x) = \pi(x) \log x + O(x^{1/2} \log x) - \int_1^x \frac{\pi(t)}{t} dt$. Note that we have $\pi(t) \ll \frac{t}{\log t}$, so we can bound the above by 
\[
\psi(x) &= \pi(x)\log x + O(x^{1/2} \log x) + O(\int_1^x \frac{1}{\log t} dt)\\
&= \pi(x) \log x+O(\frac{x}{\log x})
\]
(For $\pi(t) \ll \frac{t}{\log t}$, note that from $\pi(t) \leq t$, $\psi(x) = \pi(x)\log x + O(x^{1/2}\log x) + O(x)$. So $\pi(x) \log x = O(x)$).
\end{proof}
\end{lemma}

\begin{lemma} (9)\\
$\sum_{p \leq x} \frac{1}{p} = \log\log x + b + O(\frac{1}{\log x})$, where $b$ is some constant.
\begin{proof}
We use partial summation. Let $A(x) = \sum_{p \leq x} \frac{\log p}{p} = \log x + R(x)$ (so $R(x) \ll 1$ (by lemma 7)). Then
\[
\sum_{2 \leq p \leq x} \frac{1}{p} &= \frac{A(x)}{\log x} - \int_2^x \frac{A(t)}{t(\log t)^2} dt\\
&= 1 + O(\frac{1}{\log x}) + \int_2^x \frac{1}{t\log t} dt + \int_2^x \frac{R(t)}{t(\log t)^2} dt
\]
Note $\int_2^\infty \frac{R(t)}{t(\log t)^2} dt$ exists, say $=c$. Then
\[
\sum_{2 \leq p \leq x} \frac{1}{p} &= 1+c+O(\frac{1}{\log x}) + \log \log x - \log \log 2 + O(\int_x^\infty \frac{1}{t(\log t)^2} dt)\\
&= \log \log x + b + O(\frac{1}{\log x})
\]
\end{proof}
\end{lemma} 

\begin{thm} (10, Chebyshev)\\
If $\pi(x) \sim c \frac{x}{\log x}$, then $c=1$.\\
Note that this is weaker than PNT itself: this only says that \emph{if} that relation exists, then we must have $c=1$.\\
(Also, if $\pi(x) \sim \frac{x}{\log x - A(x)}$, then $A\sim 1$)
\begin{proof}
Use partial summation on $\sum_{p \leq x} \frac{1}{p}$:
\[
\sum_{p \leq x} \frac{1}{p} = \frac{\pi(x)}{x} - \int_1^x \frac{\pi(t)}{t^2} dt
\]
If $\pi(x) = (c+o(1)) \frac{x}{\log x}$, then
\[
&= \frac{c}{\log x} + o(\frac{1}{\log x}) + (c+o(1)) \int_1^x \frac{1}{t\log t} dt\\
&= O(\frac{1}{\log x}) + (c+o(1)) \log \log x
\]
But $\sum_{p \leq x} \frac{1}{p} = (1+o(1))\log x$. Hence $c=1$.
\end{proof}
\end{thm}

\begin{lemma} (11)\\
$\prod_{p \leq x} (1-\frac{1}{p})^{-1} = c\log x + O(1)$, where $c$ is some constant.
\begin{proof}
\[
\log (\prod_{p \leq x} (1-\frac{1}{p})^{-1}) &= -\sum_{p \leq x} (1-\frac{1}{p})\\
&= \sum_{p \leq x} \sum_k \frac{1}{kp^k}\\
&= \sum_{p \leq x} \frac{1}{p} + \sum_{k \geq 2} \sum_{p \leq x} \frac{1}{kp^k}\\
&=\log \log x + c' + O(\frac{1}{\log x})
\]
where we used the expansion $\log(1-t) = -\sum_k \frac{t^k}{k}$.\\
Now note that $e^x = 1 + O(x)$ for $|x| \leq 1$. So
\[
\prod_{p \leq x} (1-\frac{1}{p})^{-1} &= c\log x e^{O(\frac{1}{\log x})}\\
&= c\log x(1+O(\frac{1}{\log x}))\\
&= c\log x + O(1)
\]
\end{proof}
\end{lemma}
It turns out that $c=e^\gamma \approx 1.78...$, where $\gamma$ is the Euler constant that we've seen previously.

So why is PNT hard, given that we've proved so many results? From probabilistic heuristic, we have the 'probability' that $p|n$ is $\frac{1}{p}$.\\
What is the probability that $n$ is prime then? $n$ is prime iff $n$ has no prime divisors $\leq n^{1/2}$. Our guess is that the events 'divisible by $p$' are independent, so the probability that $n$ is prime should be something like $\prod_{p \leq n^{1/2}}(1-\frac{1}{p}) \approx \frac{1}{c \log n^{1/2}} = \frac{2}{c} \frac{1}{\log n}$. So
\[
\pi(x) = \sum_{n \leq x} 1_{n \ prime} \approx \frac{2}{c} \sum_{n \leq x} \frac{1}{\log n} \approx \frac{2}{c} \frac{x}{\log x} \approx 2 e^{-\gamma} \frac{x}{\log x}
\]
if the above guesses are correct; but from theorem 10 we know that the constant should be 1 instead of $2e^{-\gamma} \approx 1.122...$.\\
What have gone wrong? It turns out that the error terms accumulated are too overwhelming that they've actually contributed to the main term. So PNT is not something like we find the main term and prove that the error terms are negligible.

Recall that $1*\Lambda = \log$, so $\mu *\log = \Lambda$. So
\[
\psi(x) &= \sum_{n \leq x} \Lambda(n)\\
&= \sum_{ab \leq x} \mu(a) \log b\\
&= \sum_{a \leq x} \mu(a) (\sum_{b \leq \frac{x}{a}} \log b)
\]

Recall that
\[
\sum_{m \leq x} \log m &= x \log x - x + O(\log x),\\
\sum_{m \leq x} \tau(m) &= x\log x + (2\gamma - 1) x + O(x^{1/2})
\]
so their main terms agree.\\
So 
\[
\psi(x) &= \sum_{a \leq x} \mu(a) \left(\sum_{b \leq \frac{x}{a}} \tau(b) + 2\gamma \frac{x}{a} + O\left(\frac{x^{1/2}}{a^{1/2}}\right)\right)\\
&= \sum_{ab \leq x} \mu(a) \tau(b)\\
&= \sum_{abc \leq x} \mu(a)\\
&= \sum_{b \leq x} \sum_{ac \leq x/b} \mu(a)\\
&= \sum_{b \leq x} \sum_{d \leq x/b} \mu * 1(d)\\
&= \lfloor x \rfloor = x+O(1)
\]
since we know the last term is $0$ unless $d$ is 1.

The error term:
\[
-2\gamma \sum_{a \leq x} \mu(a) \frac{x}{a} = O(x \sum_{a \leq x} \frac{\mu(a)}{a})
\]
so we need to show that $\sum_{a \leq x} \frac{\mu(a)}{a} = o(1)$. However, this is still the same as PNT, so we haven't gained anything.

---Lecture 5---

\subsection{Selberg's identity, and an elementary proof of the PNT}

Recall that PNT is 
$$\psi(x) = \sum_{n \leq x} \Lambda(n) = x+o(x)$$

Let (\emph{Selberg's function})
$$\Lambda_2(n) = \mu*(\log^2)(n) = \sum_{ab = n} \mu(a) (\log b)^2$$
(Recall $\Lambda = \mu*\log$).

The idea is to prove a 'PNT for $\Lambda_2$' with elementary methods.

\begin{lemma} (12)\\
(1) $\Lambda_2(n) = \Lambda(n) \log n + \Lambda * \Lambda (n)$;\\
(2) $0 \leq \Lambda_2(n) \leq (\log n)^2$;\\
(3) If $\Lambda_2(n) \neq 0$, then $n$ has at most 2 distinct prime factors.
\begin{proof}
For (1), we use M\"{o}bius inverison, so it is enough to show that
\[
\sum_{d|n} (\Lambda(d) \log d + \Lambda * \Lambda(d)) &= (\log n)^2\\
&= \sum_{d | n} \Lambda(d) \log d + \sum_{ab | n} \Lambda(a) \Lambda(b) \text{ as } 1*\Lambda = \log\\
&= \sum_{d|n}\Lambda(d) \log d + \sum_{a|n} \Lambda(a)\underbrace{\left(\sum_{b|\frac{n}{a}} \Lambda(b))\right)}_{=\log(\frac{n}{d})}\\
&=\sum_{d|n} \Lambda(d) \log d + \sum_{d|n} \Lambda(d) \log (\frac{n}{d})\\
&= \log n \sum_{d|n} \Lambda(d) = (\log n)^2
\]
For (2), $\Lambda_2(n) \geq 0$ since both terms on RHS in (1) are $\geq 0$, and since $\sum_{d|n} \Lambda_2(d) = (\log n)^2$, $\Lambda_2(n) \leq (\log n)^2$.\\
For (3), note that if $n$ is divisible by 3 distinct primes, then $\Lambda(n) = 0$, and $\Lambda *\Lambda(n) = \sum_{ab = n} \Lambda(a)\Lambda(b) = 0$ since at least one of $a$ or $b$ has $\geq 2$ distinct prime divisors.
\end{proof}
\end{lemma}

\begin{thm} (13, Selberg)\\
\[
\sum_{n \leq x} \Lambda_2(n) = 2x\log x + O(x)
\]
\begin{proof}
\[
\sum_{n \leq x} \Lambda_2(n) &= \sum_{n \leq x} \mu*(\log)^2(n)\\
&= \sum_{ab \leq x} \mu(a)(\log b)^2\\
&= \sum_{a \leq x} \mu(a) \left(\sum_{b \leq \frac{x}{a}}(\log b)^2\right)
\]
By PS,
\[
\sum_{m \leq x} (\log m)^2 = x (\log x)^2 - 2x\log x + 2x + O((\log x)^2)
\]
By PS, (let $A(t) = \sum_{n \leq t} \tau(n) = t\log t + ct + O(t^{1/2})$)
\[
\sum_{m \leq x} \frac{\tau(m)}{m} &= \frac{A(x)}{x} + \int_1^x \frac{A(t)}{t^2} dt\\
&=\log x + c + O(x^{-1/2}) + \int_1^x \frac{\log t}{t} dt + c\int_1^x\frac{1}{t} dt + O(\int_1^x \frac{1}{t^{3/2}} dt)\\
&= \frac{(\log x)^2}{2} + c_1 \log x + c_2 + O(x^{-1/2})
\]
So
\[
\frac{x(\log x)^2}{2} = \sum_{m \leq x} \tau(m) \frac{x}{m} + c_1' \sum_{m \leq x} \tau(m) + c_2'x + O(x^{1/2})
\]
So
\[
\sum_{n \leq x} (\log m)^2 = 2\sum_{m \leq x} \tau(m) \frac{x}{m} + c_3 \sum_{m \leq x} \tau(m) + c_4 x + O(x^{1/2})
\]
So
\[
\sum_{n \leq x} \Lambda_2(n) = 2 \sum_{a \leq x} \mu(a) \sum_{b \leq \frac{x}{a}} \frac{\tau(b) x}{ab} + c_5 \sum_{a \leq x} \mu(a) \sum_{b \leq \frac{x}{a}} \tau(b) + c_6 \sum_{a \leq x} \mu(a) \frac{x}{a} + O(\sum_{a \leq x} \frac{x^{1/2}}{a^{1/2}})
\]
First, note that $x^{1/2} \sum_{a \leq x} \frac{1}{a^{1/2}} = O(x)$ (by PS or just comparing with the integral. Secondly,
\[
x \sum_{a \leq x} \frac{\mu(a)}{a} &= \sum_{a \leq x} \mu(a) \lfloor \frac{x}{a} \rfloor + O(x)\\
&= \sum_{a \leq x} \mu(a) \sum_{b \leq \frac{x}{a}} 1 + O(x)\\
&= \sum_{d \leq x} \mu*1 (d) + O(x)\\
&= O(x)
\]
since the sum is either 1 (when $d=1$) or 0 (otherwise).\\
Thirdly,\footnote{Jaspal noticed that this can be obtained much more easily by $\mu*\tau = 1$ from M\"{o}bius inversion.}
\[
\sum_{a \leq x} \mu(a) \sum_{b \leq x} \tau(b) &= \sum_{a\leq x} \mu(a) \sum_{b \leq \frac{x}{a}} \sum_{cd = b} 1\\
&= \sum_{a \leq x} \mu(a) \sum_{cd \leq \frac{x}{a}}1\\
&= \sum_{acd \leq x} \mu(a)\\
&= \sum_{d \leq x} \sum_{ac \leq \frac{x}{d}} \mu(a)\\
&= \sum_{d \leq x} \sum_{e \leq \frac{x}{d}} \mu* 1(e)\\
&= \sum_{d \leq x} 1 = O(x)
\]
So
\[
\sum_{n \leq x} \Lambda_2(n) &= 2\sum_{a \leq x} \mu(a) \sum_{b \leq \frac{x}{a}} \frac{\tau(b)x}{ab} + O(x)\\
&= 2x \sum_{d \leq x} \frac{1}{d} \mu * \tau(d) + O(x)
\]
Recall that $\tau = 1*1$, so $\mu *\tau = \mu*1*1=1$. So the above
\[
&= 2x \sum_{d \leq x} \frac{1}{d} + O(x)\\
&= 2x \log x + O(x)
\]
\end{proof}
\end{thm}

(Non-examinable from now, but lecturer still recommends us to think about it)\\
A 14-point plan to prove PNT from Selberg's identity:\\
Let $r(x) = \frac{\psi(x)}{x} - 1$, so PNT is equivalent to $\lim_{x \to \infty} |r(x)| = 0$.\\
1) Selberg's identity $\implies$
\[
r(x) \log x = -\sum_{n \leq x} \frac{\Lambda(n)}{n} r(\frac{x}{n}) + O(1)
\]
2) Considering 1) with $x$ replaced $\frac{x}{m}$, summing over $m$, show 
\[
|r(x)|(\log x)^2 \leq \sum_{n \leq x} \frac{\Lambda_2(n)}{n} |r(\frac{x}{n})| + O(\log x)
\]
3) 
\[
\sum_{n \leq x} \Lambda_2(n) = 2\int_1^{\lfloor x \rfloor} \log t dt + O(x)
\]
4-6) (Let's skip some of the steps)
\[
\sum_{n \leq x} \frac{\Lambda_2(n)}{n} |r(\frac{x}{n})| = 2\int_1^x \frac{|r(x/t)|}{t\log t} dt + O(\log x)
\]
7) Let $V(u) = r(e^u)$. Show that 
\[
u^2 |V(u)| \leq 2 \int_0^u \int_0^v |V(t)| dt dv + O(u)
\]
8) Show
\[
\limsup|r(x)| \leq \limsup \frac{1}{u} \int_0^u |V(t)| dt = \beta
\]
9-14) (!) If $\alpha>0$, then can show from 7) that $\beta < \alpha$, contradiction; so $\alpha=0$, and PNT.

\newpage

\section{Sieve Methods}

---Lecture 6---

Hand in by Monday if you want some questions (q2 and q3) to be marked.

Everyone knows how Sieve of Eratosthenes works (some demonstration by lecturer). Our interest is in using the sieve to \emph{count} things. If we apply Sieve of Eratosthenes to the interval $[1,20]$, then we get an equality between two ways of counting how many numbers are left:
\[
\pi(20)+1-\pi(\sqrt{20}) = 20-\lfloor 20/2\rfloor - \lfloor 20/3\rfloor + \lfloor 20/6\rfloor
\]
where both sides evaluate to 7.

\subsection{Setup}
We'll have the following:\\
$\bullet$ Finite set $A \subset \N$ (the set to be sifted);\\
$\bullet$ Set of primes $p$ (the set of primes we sift out by), usually all primes;\\
$\bullet$ Sifting limit $z$ (sift all primes in $P$ less than $z$)\\
$\bullet$ sifting function
\[
S(A,P;z) = \sum_{n \in A} 1_{(n,\prod_{p \in P, p < z}p) =1}
\]

Let $\prod_{p \in P, p < z} p = P(z)$. Our goal is to estimate $S(A,P;z)$.

$\bullet$ For $d$, let
\[
A_d = \{n \in A:d | n\}
\]
$\bullet$ We write $|A_d| = \frac{f(d)}{d} X + R_d$ (most textbooks use $\omega$ in place of $f$ here, our use here is to avoid confusion), where $f$ is multiplicative ($f(mn) = f(m) f(n) \forall (m,n)=1$), and $0 \leq f(d) \forall d$.\\
$\bullet$ Note that $|A| = \frac{f(1)}{1} X + R_1 = X+R_1$;\\
$\bullet$ $R_d$ is an 'error' term;\\
$\bullet$ We choose $f$ so that $f(p) = 0$ if $p \not\in P$ by convention (so in that case $R_p = |A_p|$).\\
$\bullet$ Let $W_p(z) = \prod_{p < z, p \in P} (1-\frac{f(p)}{p})$.

\begin{eg}
1) Take $A=(x,x+y] \cap \N$, $P$ the set of all primes. So $|A_d| = \lfloor \frac{x+y}{d} \rfloor - \lfloor \frac{x}{d}\rfloor = \frac{y}{d} + O(1)$.\\
Here $f(d) \equiv 1$, and $R_d = O(1)$.\\
So $S(A,P;z)= |\{x < n \leq x+y: p | n \implies p \geq z\}|$.\\
e.g. if $z \approx (x+y)^{1/2}$, then
\[
S(A,P;z) = \pi(x+y)-\pi(x) + O((x+y)^{1/2})
\]
\end{eg}

2) $A = \{1 \leq n \leq y: n \equiv a \pmod q\}$, $A_d = \{1 \leq m \leq \frac{x}{d}: dm \equiv a \pmod q\}$. This congruence only has solutions if $(d,q) | a$. So \\
$|A_d| = \frac{(d,q)}{dq} y + O((d,q))$ if $(d,q) | a$, and $=O((d,q))$ otherwise.\\
So here $X=y/q$, and $f(d) = d(d,q)$ if $(d,q) | a$, and $0$ otherwise.

3) How about twin primes, i.e. $p,p+2$ both primes? We have\\
$A=\{n(n+2): 1 \leq n \leq x\}$ (so if $p|n(n+2) \iff n \equiv 0,-2 \pmod p$);\\
$P$ is all primes except 2;\\
$|A_p| = \frac{2x}{p} + O(1)$ (so $f(p) = 2$). So $f(d) = 2^{\omega(d)}$ for $f$ to be completely multiplicative, where $\omega(d)$ denote the number of primes divisors of $d$.\\
$S(A,P;x^{1/2}) = |\{1 \leq p \leq x: p,p+2$ both prime$\} + O(x^{1/2})$. Denote the main term as $\pi_2(x)$, then as mentioned in the first lecture we expect $\pi_2(x) \approx \frac{x}{(\log x)^2}$. We will prove the upper bound using sieves.

\begin{thm} (1, Sieve of Eratosthenes Legendre)\\
$S(A,P;z) = X W_p(z) + O (\sum_{d | p(z)} R_d)$.
\begin{proof}
\[
S(A,P;z) &= \sum_{n \in A} 1_{(n,p(z)) = 1}\\
&= \sum_{n \in A} \sum_{d | (n,p(z))} \mu(d)\\
&= \sum_{n \in A} \sum_{d | n,d|p(z)} \mu(d)\\
&= \sum_{d | p(z)} \mu(d) \sum_{n \in A} 1_{d|n}\\
&= \sum_{d | p(z)} \mu(d) |A_d|\\
&= X \sum_{d | p(z)} \frac{\mu(d)f(d)}{d} + \sum_{d | p(z)} \mu(d) R_d\\
&= X \prod_{p \in P, p < z} \left(1-\frac{f(p)}{p}\right) + O(\sum_{d | p(z)} |R_d|)
\]
\end{proof}
\end{thm}

\begin{coro}
$\pi(x+y) - \pi(x) \ll \frac{y}{\log\log y}$.
\begin{proof}
In example 1, $f \equiv 1$, and $|R_d| \ll 1$, and $X=y$. So
\[
W_p(z) = \prod_{p \leq z} \left(1-\frac{1}{p}\right) \ll (\log z)^{-1}
\]
and 
\[
\sum_{d | p(z)} |R_d| \ll \sum_{d|p(z)} 1 \leq 2^z
\]
So $\pi(x+y) - \pi(x) \ll \frac{y}{\log z} + 2^z \ll \frac{y}{\log \log y}$ if we choose $z=\log y$.
\end{proof}
\end{coro}

---Lecture 7---

For the previous corollary: take $A=\{x < n <\leq x+y\}$, $p=$ all primes, $z=\log y$. Then $\frac{y}{\log \log y} \gg S(A,P;z) = |\{x<n \leq x+y: p|n \implies p \geq \log y\} \geq \pi(x+y) - \pi(x) + O(\log y)$.

\subsection{Selberg's Sieve}
Sieve of E-L: $S(A;P,z) \leq XW + O(\sum_{d | P(z)} |R_d|)$.\\
The problem is that we have to consider $2^z$ many divisors of $P(z)$, so we get $2^z$ many error terms, which forces us to only take $z=\log y$.\\

$\bullet$ We can do a different sieve, and only consider those divisors of $P(t)$ which are \emph{small}, say $\leq D$.\\
The key part of E-L was $1_{(n,P(z))} = 1 = \sum_{d|(n,P(z))} \mu(d)$.\\
For an upper bound, it's enough to use \emph{any} function $F$, s.t. 
\[
F(n) \geq \left\{ \begin{array}{ll}
1 & n=1\\
0 & else
\end{array}
\right.
\]
Selberg's observation was that if $(\lambda_i)$ is any sequence of reals with $\lambda_1 = 1$, then $F(n) = (\sum_{d|n} \lambda_d)^2$ works: $F(1) = (\sum_{d|1}\lambda_d)^2 = \lambda_1^2 = 1$.

Assumption: $0 < f(p) < p$ if $p \in P$ (remember that $|A_p| = \frac{f(p)}{p} X + R_p$).

This lets us define a new multiplicative(?) function $g$ s.t.
\[
g(p) = \left(1-\frac{f(p)}{p}\right)^{-1}-1 = \frac{f(p)}{p-f(p)}
\]

\begin{thm} (3, Selberg's Sieve)\\
\[
\forall t S(A,P;z) \leq \frac{X}{G(t,z)} + \sum_{d | P(z), d<t^2} 3^{\omega(d)} |R_d|
\]
where $G(t,z) = \sum_{d|P(z), d<t} g(d)$.\\
Recall $W=\prod_{p \in P,p \leq z} (1-\frac{f(p)}{p})$, so expected size of $S(A,P;z)$ is $XW$.\\
Note thas, as $t \to \infty$, 
\[
G(t,z) &= \sum_{d | P(z)} g(d)\\
&= \prod_{p<z} (1+g(p))\\
&= \prod_{p<z}(1-\frac{f(p)}{p})^{-1}\\
&\approx \frac{1}{W}
\]
\end{thm}

\begin{coro} (4)\\
$\forall x,y$,
\[
\pi(x+y)-\pi(x) \ll \frac{y}{\log y}
\]
\begin{proof}
As before, $A = \{x <n \leq x+y\}$, $f(p) \equiv 1$, $R_d = O(1), X=y$.\\
Now apply Selberg's sieve instead. Main term: 
\[G(z,z) &= \sum_{d|P(z),d<z} \prod_{p | d} (p-1)^{-1} \\
&= \sum_{d = p_1...p_r < z} \prod \sum_{k \geq 1}^\infty \frac{1}{p_i^k}\\
&\geq \sum_{d<z} \frac{1}{d}\\
&\gg \log z
\]
But on the other hand, the second last line also equals
\[
&=\sum_i \sum_{k \geq 1, p_1...p_r < z}^\infty \frac{1}{p_1^{k_1}...p_r^{k_r}}\\
&= \sum_{n, \text{ square free part of n } <z} \frac{1}{n}
\]
($g(p)=\frac{1}{p-1} = \frac{1}{\varphi(p)}$, so $g(d) = \frac{1}{\varphi(d)}$).
So our main term is $\ll \frac{y}{\log z}$.\\
Note that $3^{\omega(d)} \leq \tau_3(d) \ll_\varepsilon d^\varepsilon$. so error term is $\ll_\varepsilon t^\varepsilon \sum_{d < t^2} 1 \ll t^{2+\varepsilon} = z^{2+\varepsilon} (t=z)$. So
\[
S(A,P;z) \ll \frac{y}{\log z} + z^{2+\varepsilon} \ll \frac{y}{\log y}
\]
(choose $z=y^{1/3}$).
\end{proof}
\end{coro}

\begin{proof} (of Selberg's Sieve)\\
Let $(\lambda_i)$ be a sequence of reals, with $\lambda_1 = 1$, to be chosen later. Then
\[
S(A,P;z) &= \sum_{n \in A} 1_{(n,P(z)) = 1}\\
&\leq \sum_{n \in A} (\sum_{d | (n,P(z))} \lambda_d)^2\\
&= \sum_{d,e | P(z)} \lambda_d \lambda_e \sum_{n \in A} 1_{d|n,e|n}\\
&= \sum_{d,e | P(z)} \lambda_d \lambda_e |A_{[d,e]}|\\
&= X \sum_{d,e | P(z)} \lambda_d \lambda_e \frac{f([d,e])}{[d,e]} + \sum_{d,e|P(z)} \lambda_d \lambda_e R_{[d,e]}
\]
We'll choose $\lambda_d$ s.t. $|\lambda_d| \leq 1$, and $\lambda_d = 0$ if $d \geq t$. Then
\[
\left|\sum_{d,e | P(z)} \lambda_d\lambda_e R_{[d,e]}\right| &\leq \sum_{d,e < t, d,e|P(z)} |R_{[d,e]}|\\
&\leq \sum_{n | P(z),n<t^2} | R_n | \sum_{d,e} 1_{[d,e]=n}
\]
and since $\sum_{d,e} 1_{[d,e]=n} = 3^{\omega(n)}$ (think of $n=p_1...p_r$ where $d=p_1...p_k$ and $e=p_j...p_r$ for $k \geq j$; so basically for each $p_i$ we have 3 choices: it's in $d$, in $e$, or in both) ($n$ square-free).\\
Let $V = \sum_{d,e|P(z)} \lambda_d \lambda_e \frac{f([d,e])}{[d,e]}$. Write $[d,e] = abc$ where $d=ab$, $e=bc$, and $(a,b) = (b,c) = (a,c) = 1$ (this is possible because $n$ is square-free).
\end{proof}

---Lecture 8 missing---

---Lecture 9---

Example class today, 330-5pm, mr14.

To finish the proof that $\pi_2(x) \ll \frac{x}{(\log x)^2}$, we need to show $G(z,z) \gg (\log z)^2$, where $G(z,z) = \sum_{d| P(z), d<z} g(d)$ and $g(2) = 0$ and $g(p) = \frac{2}{p-2}$.\\
First note that $g(p) \geq \frac{2}{p-1}$. So if $d$ is odd and square free, then $g(d) \geq \frac{2^{\omega(d)}}{\varphi(d)}$ (note that the numerator is equal to $\tau(d)$ for square-free integers).\\
Now write
\[
G(z,z) &= \sum_{d < z, d \text{ odd, square free}} \frac{2^{\omega(d)}}{\varphi(d)}\\
&\gg \sum_{d < z, d \text{ square free}} \frac{2^{\omega(d)}}{\varphi(d)}
\]
(the even  terms added in are only at most twice of the original sum, so it doesn't change the order). Now the above
\[
&=\sum_{d < z, d \text{square free}, d=p_1...p_r} 2^{\omega(d)} \prod_{i=1}^r (1/p_i+1/p_i^2+...)\\
&= \sum_{d < z, d=em^2, e \text{ square free}} 2^{\omega(d)}/d\\
&\geq \sum_{d < z} \frac{2^{\omega(d)}}{d}
\]
by partial summation, it's enough to show that $\sum_{d<z} 2^{\omega(d)} \gg z\log z$, because then above $\gg (\log z)^2$ as required (check).

Recall that, to show $\sum_{d<z} \tau(d) \gg z\log z$, we used that $\tau = 1*1$. So similarly we want to write $2^{\omega(n)} = \sum_{d|n} \mu(d) f(n/d)$ where $f$ is multiplicative.\\
Let's do some calculation: for $n=1$ we need $\mu(1)f(1)=1$, so $f(1) = 1$. For $n=p$ a prime, $2=f(p)-f(1) = f(p)-1$, so $f(p)=3$.\\
For $n=p^2$ a prime, $2=f(p^2)-f(p)$, so $f(p^2) = 5$...

Ok this probably doesn't work. Let's try something more general, say write it as $\sum_{d|n} f(d) g(n/d)$, where $f$ is multiplicative. Then\\
$\bullet$ $n=1$: $f(1)=g(1)=1$;\\
$\bullet$ $n=p$: $f(p)+g(p)=2$;\\
$\bullet$ $n=p^2$: $g(p^2)+f(p^2)+f(p)g(p) = 2$.

Say let's try $f=\tau$. So $g(p) = 0, g(p^2) = -1, g(p^k) = 0$ $\forall k \geq 3$.

So therefore 
\[
g(n) = \left\{\begin{array}{ll}
0 & n \text{ not a square}\\
\mu(d) & n=d^2
\end{array}
\right.
\]
and $2^{\omega(n)} = \sum_{d|n} \tau(d) g(n/d)$.\\
So (remember $\sum_{b \leq x} \tau(b) = x\log x + (2\gamma - 1) x + O(\sqrt{x})$)
\[
\sum_{d<z} 2^{\omega(d)} &= \sum_{a<z} g(a) \sum_{b \leq z/a} \tau(b)\\
&= \sum_{a < z} g(a) z/a \log (z/a) + c\sum_{a<z} g(a) z/a + \underbrace{O(z^{1/2} \sum_{a<z} 1/a^{1/2})}_{\ll z}\\
&= \sum_{d < z^{1/2}} \mu(d) z/d^2 \log z - \underbrace{2\sum_{d<z^{1/2}} \mu(d) z/d^2 \log d}_{\ll z\sum_{d<z^{1/2}} \log d/d^2 \ll z} + O(z)
\]
Note $\sum_{d<z^{1/2}} \frac{\mu(d)}{d^2} = c+O(\sum_{d>z^{1/2}} 1/d^2) = c+O(1/z^{1/2})$. So
\[
\sum_{d<z} 2^{\omega(d)} = cz\log z + O(z) \gg z\log z
\]

It remains to show that $c>0$. Either:\\
(1) Note LHS can't be $O(z)$;\\
(2) calculate the first couple of terms in the series;\\
(3) Note that $c=6/\pi^2 (=\frac{1}{\zeta(2)}) > 0$.

\subsection{Combinatorial Sieve}
Selberg is just an upper bound sieve (where we considered
\[
(\sum_{d|n} \lambda_d)^2 \geq \left\{
\begin{array}{ll}
1 & n=1\\
0 & else\\
\end{array}
\right.
\]
with $\lambda_i=1$). Now consider some inclusion-exclusion principle:
\[
S(A,P;z) = |A| - \sum_p |A_p| + \sum_{p,q} |A_{p,q}| - ...
\]
The idea of combinatorial sieve is to truncate the sieve process.

\begin{lemma} (Buchstab Formula)\\
$S(A,P;z) = |A| - \sum_{p | P(z)} S(A_p,P;p)$.
\begin{proof}
First notice that $|A| = S(A,P;z) + \sum_{p | P(z)} S(A_p,P;p)$: the first term is the number of $n \in A$ s.t. $p|n, p \in P \implies p \geq z$ by definition (denote this set by $S_1$); the summand of second term is the number of $n \in A$ s.t. $n=mp$, and $q | n, q \in P \implies q > p$ (denote this set by $S_p$ for each $p$).\\
Now note that every element $n \in A$ is either in $S_1$, or has some prime divisors from $P(z)$. If $p$ is the least such prime divisor, then $n \in S_p$. So this is a partition of $A$.
\end{proof}
\end{lemma}

Similarly we could prove 
\[
W(z) = 1-\sum_{p|P(z)} \frac{f(p)}{p} W(p)
\]
as $W(z) = \prod_{p | P(z)} (1-\frac{f(p)}{p})$.

\begin{coro}
For any $r \geq 1$, 
\[
S(A,P;z) = \sum_{d | P(z), \omega(d) < r} \mu(d) |A_d|+(-1)^r \sum_{d|P(z),\omega(d) = r} S(A_d,P;l(d))
\]
where $l(d)$ is the least prime divisor of $d$.
\begin{proof}
Induction on $r$. $r=1$ is just Buchstab formula. For the inductive step, use
\[
S(A_d,P;l(d)) = |A_d| - \sum_{p \in P, p < l(d)} S(A_{dp},P;p)
\]
and
\[
&(-1)^r \sum_{d|P(z), \omega(d)=r} (|A_d| - \sum_{p \in P, p < l(d)} S(A_{pd},P;p))\\
=&\sum_{d|p(z), \omega(d)=r} \mu(d) |A_d| + (-1)^{r+1} \sum_{e|P(z),\omega(e) = r+1} S(A_e,P;l(e))
\]
In particular, note that if $r$ is even, then
\[
S(A,P;z) \geq \sum_{d | P(z), \omega(d) < r} \mu(d) |A_d|
\]
Similarly, if $r$ is odd we get a similar bound in the $\leq$ direction.
\end{proof}
\end{coro}

\begin{thm} (Brun's Pure Sieve)\\
For $r \geq 6\log \frac{1}{W(z)}$, then 
\[
S(A,P;z) = XW(z) + O(2^{-r} X + \sum_{d | P(z), d \leq z^r} |R_d|)
\]
(Compare this to Eratostehen's sieve:
\[
S(A,P;z) = XW(z)  +O(\sum_{d|P(z)} |R_d|)
\]
)

---Lecture 10---

\begin{proof}
Recall that from the iterating Buchstab Formula, we have that, for any $r \geq 1$,
\[
S(A,P;x) &= \sum_{d | P(z), \omega(d) < r} \mu(d) |A_d| + (-1)^r \sum_{d|P(z), \omega(d) = r} S(A_d,P;l(d))\\
&= X \sum_{d | P(z), \omega(d) < r} \mu(d) \frac{f(d)}{d} + \sum_{d | P(z), \omega(d) < r} \mu(d) R_d + (-1)^r \sum ...
\]
by trivial bound $0 \leq S(A_d,P;l(d)) \leq |A_d| = X\frac{f(d)}{d} + R_d$. So above
\[
S(A,P;z) &= X\sum_{d | P(z), \omega(d) < r} \mu(d) \frac{f(d)}{d} + O(\sum_{d | P(z),\omega(d)<r} |R_d| + \sum_{d | P(z), \omega(d) = r} |A_d|)
\]

By Buchstab again, applied to $W(z)$, we have

\[
W(z) = \sum_{d|P(z), \omega(d) < r} \mu(d) \frac{f(d)}{d} + (-1)^r \sum_{d | P(z), \omega(d) = r} \mu(d) \frac{f(d)}{d} W(l(d))
\]
So
\[
S(A,P;z) = XW(z) + O\left(\sum_{d|P(z), \omega(d) < r} |R_d| + \sum_{d|P(z), \omega(d) = r} |A_d| + X\sum_{d|P(z), \omega(d) = r} \frac{f(d)}{d}\right)
\]
where we just use a crude bound that $W(l(d)) < 1$.\\
Error term:
\[
&= X \sum_{d | P(z), \omega(d) = r} \frac{f(d)}{d} + \sum_{d|P(z), \omega(d) \leq r} |R_d|\\
&\leq \sum_{d|P(z), d \leq z'} |R_d|
\]
because $d|P(z) = \prod_{p \in P, p < z} P$. It remains to show that
\[
\sum_{d|P(z), \omega(d) = r} \frac{f(d)}{d} \ll 2^{-r}
\]

Note that
\[
\sum_{d|P(x),\omega(d) = r} \frac{f(d)}{d} &= \sum_{p_1...p_r, p \in P, p_i < z} \frac{f(p_1)...f(p_r)}{p_1...p_r} \leq \frac{(\sum_{p|P(z)} \frac{f(p)}{p})^r}{r!}
\]
and we use $r' \geq \frac{r^r}{e^r}$ to then get
\[
\leq (\frac{e\sum_{p|P(z)} \frac{f(p)}{p}}{r})^r \ (\dagger)
\]
Now
\[
\sum_{p|P(z)} \frac{f(p)}{p} \leq \sum_{p|P(z)}-\log(1-\frac{f(p)}{p}) = -\log W(z)
\]
So if $r \geq 2e|\log W(z)|$, then ($\dagger$) is at most 
\[
(\frac{e|\log W(z)|}{r})^r \leq 2^{-r}
\]
and we're done ($2e<6$).
\end{proof}
\end{thm}

Note that we could easily improve this by being more careful at the constants, but those are less important; we just want to have the purest form of the combinatorial sieve.

Recall Selberg's Sieve shows $\pi_2(x) \ll \frac{x}{(\log x)^2}$, and now combinatorial sieve gives both upper and lower bound, so we might think it would give a lower bound for $\pi_2(x)$. However, in the twin prime sieve setting, recall that $W(z) \asymp \frac{1}{(\log z)^2}$, so in Brun's sieve we need to take $r \gg 2 \log \log z$.\\
If $r = C \log \log z$ for $C$ large enough, then $2^{-r} X \ll \frac{X}{(\log z)^{100}}$ (safe enough); the main term is $\gg \frac{x}{(\log z)^2}$; $|R_d| \ll 2^{\omega(d)} = d^{o(1)}$, so 
\[
\sum_{d|P(z),d \leq 2^r} |R_d| \ll 2^{r+o(1)} = z^{2 \log \log z} + o(1).
\]

For this to be $o(\frac{x}{(\log z)^2})$, we need to choose $z \approx \exp((\log x)^{1/4})$. We seem to have success, but in the end when we try to relate $S(A,P;z)$ and $\pi_2(x)$, we have LHS is $\{1 \leq n \leq x: p|n(n+2)$ then $p \gg (?) z = \exp((\log x)^{1/4})\}$ ($p \gg x^{1/2}$), but when we try to get a lower bound it is impossible to remove the extra stuff we have here.

\begin{coro}
For any $z \leq \exp(o((\frac{\log x}{\log \log x})^{1/2}))$,
\[
|\{1 \leq n \leq x: p|n \implies p \geq z\}| \sim e^{-\gamma} \frac{x}{\log z}
\]
\end{coro}

\begin{rem}
1) In particular: $z = (\log x)^A$ is allowed for any $A$, but $z=x^c$ for any $c >0$ is not allowed.\\
2) In particular, we can't count primes like this ($z=x^{1/2})$. Recall heuristic from before says if this asymptotic were correct for primes, then $\pi(x) \sim 2e^{-r} \frac{x}{\log x}$ (contradicts PNT: $2e^{-\gamma} = 1.12...$).
\end{rem}

\begin{proof}
Again, use $A=\{1 \leq n \leq x\}$, so $f(d) = 1$ and $|R_d| \ll 1$. Then $W(z) = \prod_{p < z} (1-\frac{1}{p}) \sim e^{-\gamma}{\log z}$. So
\[
S(A,P;z) &= |\{1 \leq n \leq x: p | n \implies p > z\}| \\
&= e^{-\gamma} \frac{x}{\log z} + o(\frac{x}{\log z}) + O(2^{-r} x + \sum_{d | P(z), d < 2^r} |R_d|)
\]
If $r \geq 6 |\log W(z)|$, so $r \geq 100\log\log z$ is certainly fine: we have in that caes
\[
2^{-r} x \leq (\log z)^{-(\log 2) 100} x = o(\frac{x}{\log z})
\]
and, choosing $r=\lceil 100\log\log z\rceil$
\[
\sum_{d | P(z), d \leq 2^r} |R_d| \ll \sum_{d \leq z^r} 1 \ll z^r \leq 2^{500 (\log z) \log\log z}
\]
(this course is very forgiving on constants). It remains to note that if $\log z = o((\frac{\log x}{\log \log x})) = \frac{(\log x)}{(\log \log x)} F(x)$, then
\[
\log z \log\log z &= o(\frac{\log x}{\log\log x} \cdot \log \log x)\\
&= o(\log x)
\]
So $2^{500 (\log\log z)\log z} \leq x^{1/10} = o(\frac{x}{\log z})$ if $x$ is large enough.
\end{proof}

---Lecture 11---

\newpage

\section{The Riemann $\zeta$ Function}

Lecture notes is again up to date again!

Let's start with some gentle introduction.

In this chapter, and as a tradition in this area, write $s=\sigma+it$ for a complex number $s$ (instead of $z$).

If $n \in \N$, $n^s = e^{\log n}$ by definition, which is entire. It's also equal to $n^\sigma \cdot e^{it \log n}$.

The Riemann $\zeta$ function is defined for $\sigma>1$:
\[
\zeta(s) := \sum_{n=1}^\infty \frac{1}{n^s}
\]

\subsection{Dirichlet series}
For any arithmetic $f:\N \to \C$, we have a Dirichlet series 
\[
L_{f} (s) = \sum_{n=1}^\infty \frac{f(n)}{n^s}
\]
We have to be careful here because there are certainly choices of $f$ that make $L_f(s)$ not converge anywhere (e.g. $f=n!$).

\begin{lemma} (1)\\
For any $f$, there is an abscissa of convergence $\sigma_c$ s.t.\\
(1) $\sigma < \sigma_c \implies L_f(s)$ diverges;\\
(2) $\sigma > \sigma_c \implies L_f(s)$ converges uniformly in some neighbourhood of $s$. In particular, $L_f(s)$ is holomorphic at $s$.
\begin{proof}
It is enough to show if $L_f(s)$ converges at $s_0$ and $\sigma > \sigma_0$, then there's a neighbourhood of $s$ on which $L_f$ converges uniformly (then we could take $\sigma_c = \inf\{ \sigma: L_f(s)$ converges$\}$). Note that it's entirely possible that $\sigma_c=\infty$ or $\sigma_c=0$.\\
Let $R(u) = \sum_{n > u} f(n) n^{-s_0}$. By partial summation,
\[
\sum_{M < n \leq N} f(n) n^{-s} = R(M) M^{s_0-s} - R(N)N^{s_0-s} + (s_0-s) \int_M^N R(u) u^{s_0}-s-1 du
\]
If $|R(u)| \leq \varepsilon$ for all $u \geq M$, then 
\[
\left|\sum_{M \leq n \leq N} f(n) n^{-s} \right| &\leq 2\varepsilon + \varepsilon |s_0-s| \int_M^n u^{\sigma_0-\sigma-1} du\\
&\leq (2+\frac{|s_0-s|}{|\sigma_0-\sigma|}) \varepsilon
\]
Note that there is a neighbourhood of $s$ in which $\frac{|s-\sigma_0|^M}{|\sigma-\sigma_0|} \ll 1$. So $\sum \frac{f(n)}{n^3}$ converges uniformly here.
\end{proof}
\end{lemma}

\begin{lemma} (2, see Part IB complex analysis for the general case)\\
If $\sum \frac{f(n)}{n^s} = \sum \frac{g(n)}{n^s}$ for all $s$ in some half-plane $\sigma > \sigma_0 \in \R$, then $f(n) = g(n) \forall n$.
\begin{proof}
It's enough to consider $\sum \frac{f(n)}{n^s} \equiv 0 \forall \sigma > \sigma_0$.\\
Suppose $\exists n f(n) \neq 0$. Let $N$ be the least such $f(N) \neq 0$. Since $\sum_{n \geq N} \frac{f(n)}{n^\sigma} = 0$,
\[
f(N) = -N^\sigma \sum_{n \geq N} \frac{f(n)}{n^\sigma}
\]
So $|f(n)| \ll n^\sigma$; and so the series $\sum_{n >N} \frac{f(n)}{n^{\sigma+1+\varepsilon}}$ is absolutely convergent.\\
So since $\frac{f(n)}{n^\sigma} \to 0$ as $\sigma \to \infty$, RHS also $\to 0$. So $f(N) = 0$.
\end{proof}
\end{lemma}

\begin{lemma} (3)\\
If $L_f(s)$ and $L_g(s)$ are absolutely convergent at $s$, then
\[
L_{f*g} (s) = \sum_{n=1}^\infty \frac{f*g(n)}{n^s}
\]
is also absolutely convergent at $s$, and is equal to $L_f(s) L_g(s)$.
\begin{proof}
\[
\left(\sum_{n=1}^\infty \frac{f(n)}{n^s}\right)\left(\sum_{m=1}^\infty \frac{g(m)}{m^s}\right) &= \sum_{n,m=1}^\infty \frac{f(n)g(m)}{(nm)^s}\\
&= \sum_{k=1}^\infty \frac{1}{k^s} \left(\sum_{n,m,nm=k} f(n) g(m)\right)
\]
(why is the last one abs. convergent?)
\end{proof}
\end{lemma}

\begin{lemma} (4, Euler product)\\
If $f$ is multiplicative, and $L_f(s)$ is absolutely convergent at $s$, then we have a very nice alternative form of the Dirichlet series:
\[
L_f(s) = \prod_p \left(1+\frac{f(p)}{p^s} + \frac{f(p^2)}{p^{2s}} + ... \right)
\]
\begin{proof}
The informal proof is to just multiply out RHS, and by fundamental theorem of arithmetic we know every term in the product appears exactly once in LHS. But obviously we have to care about the issue of convergence here.\\
Let $y$ be arbitrary, consider the truncated product
\[
\prod_{p<y} \left(1+\frac{f(p)}{p^s} + ... \right) = \sum_{n,p|n \implies p<y} \frac{f(n)}{n^s}
\]
So use the usual technique in analysis of bounding errors,
\[
\left|\prod_{p<y}\left(1+\frac{f(p)}{p^s} + ... \right) - \sum_{n=1}^\infty \frac{f(n)}{n^s} \right| & \leq \sum_{n, p|n \implies p<y} \frac{|f(n)|}{n^\sigma} \\
&\leq \sum_{n, \exists p | n (p \geq y)} \frac{|f(n)|}{n^\sigma} \to 0
\]
as $y \to \infty$.
\end{proof}
\end{lemma}

For $\sigma>1$, $\zeta(s) = \sum_{n=1}^\infty \frac{1}{n^s}$ defines a holomorphic function and converges absolutely for $\sigma>1$. Note that it definitely can't converge for $\sigma=1$ as we get the harmonic series when $s=1$. So in this case we get a function that absolutely converges whenever it converges.\footnote{Never ever use this Dirichlet series when $s$ is not in that half plane -- else we'll discover that \emph{the sum of all natural numbers is $-\frac{1}{12}$}.}

Note that $\zeta'(s) = \sum \frac{1}{n^s})' = -\sum \frac{\log n}{n^s}$. Since $1$ is completely multiplicative,
\[
1+\frac{1}{p^s} + \frac{1}{p^{2s}} + ... = (1-p^{-s})^{-1}
\]
So
\[
\zeta(s) = \prod_p \left(\frac{1}{1-p^{-s}}\right)
\]
So
\[
\frac{1}{\zeta(s)} &= \prod_p (1-\frac{1}{p^s})\\
&= \sum_n \frac{\mu(n)}{n^s}
\]
\[
\log \zeta(s) &= -\sum_p \log(1-\frac{1}{p^s})\\
&= \sum_p \sum_k \frac{1}{kp^{ks}}\\
&= \sum \frac{\Lambda(n)}{\log n} \frac{1}{n^s}
\]
Finally if we take the derivative, we'll get
\[
\frac{\zeta'(s)}{\zeta(s)} = -\sum \frac{\Lambda(n)}{n^s}
\]
We have some interesting results from this point: for example, consider
\[
\frac{\zeta'(s)}{\zeta(s)} \times \zeta(s) = \zeta'(s)
\]
which corresponds to $\Lambda * 1 = \log$.

Similarly we have M\"{o}bius inversion: if $f*1 =g$, then $L_f \times \zeta = L_g$, so $L_f = \frac{1}{\zeta} \times L_g$, i.e. $f=\mu *g$.

---Lecture 12---

2nd es in online, as well as typed solution to the first one!

Before we start the lecture I want to properly answer a question last time. Last time we had
\[
\frac{1}{\zeta(s)} = \sum_{n=1}^{\infty} \frac{\mu(n)}{n^s}
\]
remember in chapter one we mentioned that PNT is equivalent to proving that $\sum \frac{\mu(n)}{n} = 0$. So it might be tempting to use the above and claim that LHS $\to 0$ as $s \to 1$, therefore RHS $\to 0$. The reason that this doesn't work is that it's not obvious that RHS converges at all; we only know that if RHS converges, then it converges to 0. But the hard part is to show that RHS does converge.

For $\sigma>1$, $\zeta(s) = \sum_n \frac{1}{n^s}$.

\begin{lemma}
For $\sigma>1$,
\[
\zeta(s) = 1+\frac{1}{s-1} - s\int_1^\infty \frac{\{t\}}{t^{s+1}} dt
\]
\begin{proof}
by partial summation,
\[
\sum_{1 \leq n \leq x} \frac{1}{n^s} &= \frac{\lfloor x\rfloor}{x^s} + s\int_1^x \frac{\lfloor t \rfloor}{t^{s+1}} dt\\
&= \frac{\lfloor x\rfloor}{x^s} + s\int_1^x \frac{1}{t^s} dt - s\int_1^x \frac{\{t\}}{t^{s+1}}dt\\
&= \frac{\lfloor x\rfloor}{x^s} + \frac{s}{s-1} \left[t^{-s+1}\right]_1^x - s\int_1^x \frac{\{t\}}{t^{s+1}} dt
\]
Limit as $x \to \infty$,
\[
&=0+\frac{s}{s-1} - s\int_1^\infty \frac{\{t\}}{t^{s+1}} dt
\]
\end{proof}
\end{lemma}

The integral converges absolutely for $\sigma > 0$, so this gives 
\[
\zeta(s) = \frac{1}{s-1} + F(s)
\]
where $F(s)$ is holomorphic in $\sigma>0$.\\
We \emph{define}
\[
\zeta(s) = 1+\frac{1}{s-1} - s\int_1^\infty \frac{\{t\}}{t^{s+1}} dt
\]
(for $\sigma > 0$). Note that $\zeta(s)$ is meromorphic in $\sigma>0$, with only a simple pole at $s=1$.

\begin{coro}
For $0<\sigma<1$, 
\[
\frac{1}{\sigma-1} < \zeta(\sigma) < \frac{\sigma}{\sigma-1}
\]
In particular, $\zeta(\sigma) < 0$ for $0 < \sigma < 1$ (in particular, not zero).
\begin{proof}
\[
\zeta(\sigma) = 1+\frac{1}{\sigma-1} - \sigma \int_1^\infty \frac{\{t\}}{t^{\sigma+1}} dt
\]
Now 
\[
0 < \int_1^\infty \frac{\{t\}}{t^{\sigma+1}} dt < \frac{1}{\sigma}
\]
so done.
\end{proof}
\end{coro}

\begin{coro}
For $0 < \delta \leq \sigma \leq 2$, $|t| \leq 1$, $\zeta(s) = \frac{1}{s-1} + O_\delta(1)$ uniformly.
\begin{proof}
\[
\zeta(s) - \frac{1}{s-1} &= 1 - s \int_1^\infty \frac{\{t\}}{t^{s+1}} dt\\
&= O(1) + O\left(\int_1^\infty \frac{1}{t^{\sigma+1}} dt\right)\\
&= O(1)+O_\delta(1)
\]
\end{proof}
\end{coro}

\begin{lemma}
$\zeta(s) \neq 0$ for $\sigma > 1$.
\begin{proof}
For $\sigma>1$, $\zeta(s) = \prod_p (1-\frac{1}{p^s})^{-1}$, and the infinite product converges, and no factors are zero. (so???)\\
(lecturer has some careful definition of convergence of infinite product, by taking logarithm and converting it to an infinite sum first then take exponent, so the result can't be zero -- check).
\end{proof}
\end{lemma}

It then becomes interesting if $\zeta(s)$ has any zeroes in $\sigma>0$. We have a famous conjecture:

\begin{conj}
If $\zeta(s) = 0$ and $\sigma > 0$, then $\sigma = \frac{1}{2}$.
\end{conj}

\subsection{Prime Number Theorem}

Let $\alpha(s) = \sum \frac{a_n}{n^s}$. Partial summation lets us write $\alpha(s)$ in terms of $A(x) = \sum_{n \leq x} a_n$.\\
If $\sigma > \max(0,\sigma_c)$(complex part of $\sigma$?) then $\alpha(s) = s \int_1^\infty \frac{A(t)}{t^{s+1}} dt$ (this is called the \emph{Mellin transform}).

What about the converse? Note that if $\alpha(s) = \frac{\zeta'(s)}{\zeta(s)}$, then $a_n = \Lambda(n)$ so $A(x) = \sum_{n \leq x} \Lambda(n) = \psi(x)$.

Converse -- Perran's formula:

\[
A(x) = \frac{1}{2\pi i} \int_{\sigma-i\infty}^{\sigma+i\infty} \alpha(s) \frac{x^s}{s} ds
\]
for $\sigma > \max(0,\sigma_c)$.

Note that 
\[
\psi(x) = \frac{1}{2\pi i} \int_{\sigma-i\infty}^{\sigma+i\infty} -\frac{\zeta'(s)}{\zeta(s)} \frac{x^s}{s} ds
\]
for $\sigma > 1$. We could try a contour to the left going past the line $\sigma=1$,and we know there's a pole at $s=1$ with residue $x$, so by residue theorem (see part IB Complex Method) it gives our main term for PNT (recall PNT is $\psi(x) = x+o(x)$), but the problem is that there might be other poles, which would be especially bad if they are on the line $\sigma=1$ as that cancels our main term. That's why we say \emph{PNT is equivalent to no zeros of $\zeta(s)$ on $\sigma=1$}.

\begin{lemma} ((Pre)-Perran's Formula)\\
If $\sigma > 0, y\neq 1$, then 
\[
\frac{1}{2\pi i} \int_{\sigma-iT}^{\sigma+iT} \frac{y^s}{s} ds = 
\left\{
\begin{array}{ll}
1 & y>1\\
0 & y<1
\end{array}
\right.
+ O\left(\frac{y^{\sigma}}{T|\log y|}\right)
\]

---Lecture 13---

\begin{proof}
Use the contour $c$ (see diagram) for $y>1$.\\
Since $y^s/s$ has a single pole at $s=0Z$ with residue $1$, so by the residue theorem, $\frac{1}{2\pi i} \int_c y^s/s ds = 1$.\\
Now we bound
\[
\int_{P_1} \frac{y^s}{s} ds = \int_{-\infty}^\sigma \frac{y^{u+iT}}{u+iT} du \ll \frac{1}{T} \int_{-\infty}^\sigma y^u du = \frac{y^\sigma}{T\log y}
\]
Similarly, $\int_{P_2} \ll \frac{y^\sigma}{T\log y}$, sot $\int_c = \int_{\sigma-iT}^{\sigma+iT} + O(\frac{y^\sigma}{T\log y})$.\\
For $y<1$, use the same argument with the contour being the rectangle going right. There are no poles inside this region, so the main term is 0, and the error term can be bounded similarly.
\end{proof}
\end{lemma}

\begin{thm} (Perran's formula)\\
Suppose $\alpha(s) = \sum \frac{a_n}{n^s}$ is absolutely convergent for all $\sigma > \sigma_a$.\\
If $\sigma_0 > \max(0,\sigma_a)$, and $x$ is not an integer, then
\[
\sum_{n < x} a_n = \frac{1}{2\pi i} \int_{\sigma_0-iT}^{\sigma_0+iT} \alpha(s) \frac{x^s}{s} ds + O(\frac{2^{\sigma_0}x}{T} \sum_{\frac{x}{2} < n < 2x} \frac{|a_n|}{|x-n|} + \frac{x^{\sigma_0}}{T} \sum_{n=1}^\infty \frac{|a_n|}{n^{\sigma_0}})
\]
\begin{proof}
Since $\sigma_0>0$, we can write
\[
1_{n<x} = \frac{1}{2\pi i} \int_{\sigma_0-iT}^{\sigma_0+iT} \frac{(x/n)^s}{s} ds + O(\frac{(x/n)^{\sigma_0}}{T|\log (x/n)|})
\]
Now 
\[
\sum_{n < x} a_n &= \frac{1}{2\pi i} \sum_n \frac{a_n}{n^s} \int_{\sigma_0-iT}^{\sigma_0+iT} \frac{x^s}{s} ds + O(\frac{x^{\sigma_0}}{T} \sum_n \frac{|a_n|}{n^{\sigma_0}|\log(\frac{x}{n})|})\\
&= \frac{1}{2\pi i} \int_{\sigma_0 - iT}^{\sigma_0+iT} \frac{x^s}{s} \sum_n \frac{a_n}{n^s} ds + O(...)\\
&= \frac{1}{2\pi i} \int_{\sigma_0-iT}^{\sigma_0+iT} \alpha(s) \frac{x^s}{s} ds + O(...)
\]
For the error term:\\
(1) Contribution from $n \leq \frac{x}{2}$ or $n \geq 2x$, where $|\log (\frac{x}{n})| \gg 1$ is $\ll \frac{x^{\sigma_0}}{T} \sum_n \frac{|a_n|}{n^{\sigma_0}}$.\\
(2) Contribution from $\frac{x}{2}<n<2x$, we write
\[
|\log(\frac{x}{n})| = |\log (1+\frac{n-x}{x})|
\]
and $|\log (1+\delta)| \asymp |\delta|$ uniformly for $-\frac{1}{2} \leq \delta \leq 1$. So
\[
\frac{x^{\sigma_0}}{T} \sum_{\frac{x}{2} < n < 2x} \frac{|a_n|}{n^{\sigma_0} |\log (x/n)|} &\ll \frac{x^\sigma_0}{T} \sum_{\frac{x}{2} < n < 2x} \frac{|a_n|x}{n^{\sigma_0}|x-n|}\\
&\ll \frac{2^{\sigma_0}}{T} \sum_{x/2<n<2x} \frac{|a_n|x}{|x-n|}
\]
\end{proof}
\end{thm}

We'll now prove a strong form of the PNT, assuming the following results which we'll prove in the future:\\
(1) $\exists c > 0$ s.t. if $\sigma > 1-\frac{c}{\log(|t|+4)}$ and $|t| \geq \frac{7}{8}$, then $\zeta(s) \neq 0$, and $\frac{\zeta'}{\zeta}(s) \ll \log(|t|+4)$ (the $+4$ is just to make sure the denominator is not negative);\\
(2) $\zeta(s) \neq 0$ for $\frac{8}{9} \leq \sigma \leq 1$ and $|t| \leq \frac{7}{8}$;\\
(3) $\frac{\zeta'}{\zeta}(s) = \frac{-1}{s-1} + O(1)$ for $1-\frac{c}{\log(|t|+4)} < \sigma \leq 2$ and $|t| \leq \frac{7}{8}$.
Let's draw a picture of what we're assuming here: (diagram)\\

\begin{thm} (PNT)\\
There exists $c > 0$ s.t.
\[
\psi(x) = x+O(\frac{x}{\exp(c\sqrt{\log x})})
\]
In particular, $\psi(x) \sim x$.
\begin{proof}
Assume that $x = N+\frac{1}{2}$ for some $N$ (for convenience). By Perran's formula, for any $1 < \sigma_0 \leq 2$,
\[
\psi(x) &= \sum_{n \leq x} \Lambda(n)\\
&= \frac{1}{2\pi i} \int_{\sigma_0-iT}^{\sigma_0+iT} -\frac{\zeta'}{\zeta}(s) \frac{x^s}{s} ds + O(\underbrace{\frac{x}{T} \sum_{x/2 < n 2x} \frac{\Lambda(n)}{|x-n|}}_{=R_1} + \underbrace{\frac{x^{\sigma_0}}{T} \sum \frac{\Lambda(n)}{n^{\sigma_0}}}_{=R_2})
\]
In the error term,
\[
R_1 &\ll \log x \frac{x}{T} \sum_{\frac{x}{2} < n \leq 2x} \frac{1}{|x-n|}\\
&\ll \log x \frac{x}{T} \sum_{1 \leq m \leq 4x} \frac{1}{m} \\
&\ll \frac{x}{T} (\log x)^2
\]
and using point 3),
\[
R_2 &\ll \frac{x^{\sigma_0}}{T} \frac{1}{|\sigma_0-1|}\\
&\ll \frac{x}{T} \log x
\]
if $\sigma_0 = 1+\frac{1}{\log x}$.\\
Let $c$ be the contour (see diagram), where we used $\sigma_1 = 1-\frac{c}{\log T} < 1$. Then 
\[
\frac{1}{2\pi i} \int_c -\frac{\zeta'(s)}{\zeta(s)} \frac{x^s}{s} ds = x
\]
by residue theorem, (1) and (2) (ensuring that there are no other poles).\\
Now we need to bound the error terms:
\[
\int_{\sigma_0+iT}^{\sigma_1+iT} -\frac{\zeta'(s)}{\zeta(s)} \frac{x^s}{s} ds &\ll \log T \int_{\sigma_0}^{\sigma_1} \frac{x^u}{T} du\\
&\ll \frac{\log T}{T} x^{\sigma_1}(\sigma_1-\sigma_0)\\
&\ll \frac{x}{T}
\]
For the left part,
\[
\int_{\sigma_1-iT}^{\sigma_1+iT} -\frac{\zeta'(s)}{\zeta(s)} \frac{x^s}{s} ds &\ll (\log T) |\int_{\sigma_1\pm iT}^{\sigma_1\pm i} \frac{x^u}{u} du| + (\int_{\sigma_1-i}^{\sigma_1+i} x^{\sigma_1} \frac{1}{|\sigma_1-1|})\\
&\ll x^{\sigma_1} \log T + \frac{x^{\sigma_1}}{1-\sigma_1}\\
&\ll x^{\sigma_1}(\log T)
\]
So
\[
\psi(x) &= x+O(\frac{x}{T}(\log x)^2 + x^{1-\frac{c}{\log T}} (\log T))\\
&= x+O(\frac{x}{\exp(c\sqrt{\log x})})
\]
if we choose $T=\exp(c\sqrt{\log x})$.\\
(some insight on why we choose this bound: we want $\frac{x}{T} \approx x^{1-\frac{c}{\log T}}$, i.e. $\log T \approx \frac{\log x}{\log T}$. Therefore $\log T \approx \sqrt{\log x}$).
\end{proof}
\end{thm}

---Lecture 14---

\subsection{Zero-free region}

Firstly, near $s=1$, things are easy because of the pole.

\begin{thm}
If $\sigma > \frac{1+t^2}{2}$, then $\zeta(s) \neq 0$. (a region enclosed by a parabola opened to the right, see diagram) In particular, $\zeta(s) \neq 0$ if $\frac{8}{9} \leq \sigma \leq 1$, $|t| \leq \frac{7}{8}$.\\
Also, $\zeta(s) = \frac{1}{s-1}+O(1)$, and $-\frac{\zeta'}{\zeta}(s) = \frac{1}{s-1} + O(1)$ uniformly in $8/9 \leq \sigma \leq 2$ and $|t| \leq 7/8$.
\begin{proof}
Recall that $\zeta(s) = \frac{s}{s-1} + s \int_1^\infty \frac{\{u\}}{u^{s+1}} du$, so 
\[
|\zeta(s)-\frac{s}{s-1}| & \leq |s| \int_1^\infty \frac{1}{u^{\sigma+1}} du\\
&\leq \frac{|s|}{\sigma}
\]
so if $\sigma > |s-1|$, $\zeta(s) \neq 0$, i.e. if $\sigma < \frac{1+t^2}{2}$; and in particular $\frac{1+(7/8)^2}{2} < 8/9$.\\
Also,
\[
|\zeta(s) - \frac{1}{s-1}| \leq 1+|s| \int_1^\infty \frac{1}{u^{\sigma+1}} du = O(1)
\]
So same holds for $-\frac{\zeta'}{\zeta}$.
\end{proof}
\end{thm}

The main reason we could obtain the above is that the pole at $1$ is convenient: it dominates everything around it, so that's the only thing we care.

For $|t|$ large, we need a different idea. How do we show that there aren't zeros on $\sigma=1$?\\
Suppose there is a zero, of order $m$, at $1+it$. Then (by expanding it at $1+it$)
\[
\frac{\zeta'}{\zeta}(1+\delta+it) \sim \frac{m}{\delta}
\]
So
\[
\sum \frac{\Lambda(n)}{n^{1+\delta+it}} \sim -\frac{1}{\delta}
\]
But
\[
|LHS| \leq \sum \frac{\Lambda(n)}{n^{1+\delta}} = -\frac{\zeta'}{\zeta} (1+\delta) \sum \frac{1}{\delta}
\]
So
\[
\sum_p \frac{\log p}{p^{1+\delta}} e^{it\log p} \sim -\sum \frac{\log p}{p^{1+\delta}}
\]
So
\[
\cos(t\log p) \approx -1
\]
for almost all primes $p$. So $p^{it} \approx -1$ for almost every prime; so $p^{2it} \approx 1$. So there must be a pole at $1+2it$; contradiction.

Let's now forget about number theory and look at some purely complex analytical results:
\begin{lemma} (Borel-Caratheodery Lemma)\\
Suppose $f$ is holomorphic on $|z| \leq R$ and $f(0) = 0$.\\
If $\Re f(z) \leq M$ for all $|z| \leq R$ for any $r < R$,
\[
\sup_{|z| \leq r} (|f(z)|,|f'(z)|) \ll_{r,R} M
\]
\begin{proof}
Let $g(z) = \frac{f(z)}{z(2M-f(z))}$. This is holomorphic in $|z| \leq R$ (note that $2M-f(z) \neq 0$, and the zero at $z=0$ is cancelled by our assumption $f(z) = 0$).\\
If $|z| = R$, then $|2M-f(z)| \geq |f(z)$, and so 
\[
|g(z)| \leq \frac{|f(z)|}{R|f(z)|} \leq \frac{1}{R}
\]
So for all $|z| \leq r M R$, by maximum modulus,
\[
|g(z)| = \frac{|f(z)|}{|z||2M-f(z)|} \leq \frac{1}{R}
\]
So
\[
R|f(z)| \leq r|2M-f(z)| \leq 2Mr+r|f(z)|
\]
so
\[
|f(z)| \leq \frac{2Mr}{R-r} \ll M
\]
This proves the claim for $f$. For $f'(z)$, we use Cauchy's formula,
\[
f'(z) = \frac{1}{\pi i} \int_{|w|=r'} \frac{f(w)}{(z-w)^2} dw \ (r < r' < R)
\]
So $f'(z) \ll M$ as well.
\end{proof}
\end{lemma}

\begin{lemma}
If $f$ is holomorphic on a domain including $|z| \leq 1$, $|f(z)| \leq M$ in this disc, and $f(0) \neq 0$. If $0<r<R<1$, then for $|z| \leq r$,
\[
\frac{f'}{f}(z) = \sum_{k=1}^K \frac{1}{z-z_k} + O_{r,k}(\log \frac{M}{|f(0)|})
\]
where $z_k$ ranges over all zeros of $f$ in $|z| \leq R$.
\begin{proof}
Suppose that $f(0) = 1$ (WLOG). Say first there are no zeros; consider $h(z) = \log f(z)$. Now $\Re h(z) = \log |f(z)| \leq \log M$, so by Borel-Caratheodery lemma,
\[
|h'(z)| = |\frac{f'}{f}(z)| \ll \log M
\]
so done.\\
In general, we define an auxiliary function $g$, with no zeros,
\[
g(z) = f(z) \prod_{k=1}^K \frac{R^2 - z\bar{z}_k}{(z-z_k)R}
\]
The $k^{th}$ factor has a pole at $z=z_k$, and on $|z|=R$, has modulus 1.\\
So on $|z| \leq R$, $|g(z)| \leq M$, in particular, 
\[
|g(0)| = \prod_{k=1}^K \frac{R}{|z_k|} \leq M \ (*)
\]
Now let $h(z) = \log \frac{g(z)}{g(0)}$, which is well-defined now. We know $\Re h(z) = \log |g(z)| - \log |g(0)| \leq \log M$ for $|z| \leq R$. By B-C lemma,
\[
|h'(z)| = \left|\frac{f'}{f}(z) - \sum_{k=1}^K \frac{1}{z-z_k} + \sum_{k=1}^K \frac{1}{z-R^2/\bar{z}_k}\right|
\]
So
\[
\frac{f'}{f}(z) = \sum_{k=1}^K \frac{1}{z-z_k} - \sum_{k=1}^K \frac{1}{z-R^2/\bar{z}_k} + O(\log M)
\]
and if $|z| \leq r$,
\[
|z-R^2/\bar{z}_k| \geq \frac{|R^2|}{|z_k|} - |z| \geq R-r \gg 1
\]
and $K \ll \log M$ (coming from (*)?).
\end{proof}
\end{lemma}

\begin{coro}
If $|t| \geq 7/8$ and $5/6 \leq \sigma \leq 2$, then 
\[
\frac{\zeta'}{\zeta}(s) = \sum_\rho \frac{1}{s-\rho} + O(\log |t|)
\]
where the sum of $\rho$ is over all zeros in $|\rho - (3/2+it)| \leq 5/6$.
\end{coro}

---Lecture 15---

Example Class today at 330-5pm in MR14!

(a diagram drawn for the corollary last time)

\begin{thm}
There is $c>0$ s.t. $\zeta(s) \neq 0$ if $\sigma \geq 1-\frac{c}{\log t}$.
\begin{proof}
Assume $\zeta(\rho) = 0$, $\rho=\sigma+it$. Let $\delta > 0$ be chosen later. Use the previous corollary,
\[
\frac{\zeta'}{\zeta}(1+\delta+it) &= \frac{1}{1+\delta+it-\rho}+\sum_{\rho' \neq \rho} \frac{1}{1+\delta+it-\rho'} + O(\log t) \footnote{unless $\sigma$ is really small. So let's assume $\sigma \geq \frac{99}{100}$}\\
\Re \frac{\zeta'}{\zeta}(1+\delta+it) &= \Re \frac{1}{1+\delta+it-\rho} + \Re \sum_{\rho' \neq \rho} \frac{1}{1+\delta+it-\rho'} + O(\log t)\\
&= \frac{1}{1+\delta-\sigma} + O(\log t) + ...
\]
Since $\Re \rho' \leq 1$, $\Re \frac{1}{1+\delta+it-\rho'} > 0$. So
\[
\Re \frac{\zeta'}{\zeta}(1+\delta+it) > \frac{1}{1+\delta-\sigma} + O(\log t)
\]
Similarly we know
\[
\Re \frac{\zeta'}{\zeta}(1+\delta+2it) > O(\log t)
\]
Also,
\[
\frac{\zeta'}{\zeta}(1+\delta) = \frac{-1}{\delta} + O(1)
\]
The next step is basically magic (that's why it took so many years for this zero-free region to be proven):
\[
&\Re\left(-3\frac{\zeta'}{\zeta}(1+\delta) - 4\frac{\zeta'}{\zeta}(1+\delta+it) - \frac{\zeta'}{\zeta} (1+\delta+2it)\right)\\
<&\frac{3}{\delta}+O(1) - \frac{4}{1+\delta-\sigma}+O(\log t)
\]
So intuitively if we make $\sigma \to 1$ and $\delta \to 0$, the above is negative. However, above is also equal to a dirichlet series,
\[
&=\Re\left(3\sum_n \frac{\Lambda(n)}{n^{1+\delta}}+4\sum_n \frac{\Lambda(n)}{n^{1+\delta+it}}+\sum_n \frac{\Lambda(n)}{n^{1+\delta+2it}}\right)\\
&=\sum_n \frac{\Lambda(n)}{n^{1+\delta}}\left(3+4\cos(t\log n) + \cos (2t\log n)\right)
\]
Note that $3+4\cos \theta + \cos(2\theta) = 2(1+\cos\theta)^2 \geq 0$. So each term in the above sum is non-negative. \\
Now we only need to properly choose $\delta$ so that the above is really negative (to obtain a contradiction): we need 
\[
\frac{3}{\delta} > \frac{4}{1+\delta-\sigma}+O(\log t)
\]
so choose $\delta = c/\log t$ for large enough $c$, so 
\[
\frac{4}{1+\delta-\sigma} < \frac{10}{8} \implies \sigma \geq 1-\frac{c}{\log t}
\]
\end{proof}
\end{thm}

The best known to date is $\sigma \geq 1-\frac{c(\log \log t)^{1/3}}{(\log t)^{2/3}}$, but it's still going to $0$ as $t \to \infty$.

\begin{lemma}
If $\sigma > 1-\frac{c}{2\log t}$, and $|t| \geq \frac{7}{8}$, then
\[
\frac{\zeta'}{\zeta}(s)| \ll \log t
\]
\begin{proof}
Let $s_1 = 1+\frac{1}{\log t} + it = \sigma_1+it$. Here
\[
\left|\frac{\zeta'}{\zeta}(s_1)\right| \ll \sum_{n=1}^{\infty} \frac{\Lambda(n)}{n^{\sigma_1}} \ll \frac{1}{\sigma_1-1} \ll \log t
\]
We have
\[
\frac{\zeta'}{\zeta}(s_1) = \sum_\rho \frac{1}{s_1-\rho} + O(\log t)
\]
therefore, if we take real part everywhere, then
\[
\Re \sum_\rho \frac{1}{s_1-\rho} \ll \log t
\]
Now if $s=\sigma+it$, where $\sigma > 1-\frac{c}{2\log t}$ (where $c$ is the same as in the zero-free region theorem above), then
\[
\frac{\zeta'}{\zeta}(s) - \frac{\zeta'}{\zeta}(s_1) = \sum_\rho (\frac{1}{s-\rho} - \frac{1}{s_1-\rho}) + O(\log t)
\]
Note that we're implicitly using the zero-free region, because otherwise $s$ might be one of the $\rho$ so our sum won't make sense.\\
Also, $|s-\rho| \asymp |s_1-\rho|$, so
\[
\left|\frac{1}{s-\rho} - \frac{1}{s_1-\rho}\right| \ll \frac{1}{|s_1-\rho|^2 \log t} \ll \Re \frac{1}{s_1-\rho} \ (\text{as }\Re (\frac{1}{z}) = \frac{\Re z}{|z|^2})
\]
so
\[
\sum_\rho |\frac{1}{s-\rho} - \frac{1}{s_1-\rho}| \ll \Re \sum_\rho \frac{1}{s_1-\rho} \ll \log t
\]
\end{proof}
\end{lemma}
This concludes our components of proof of PNT.

Assuming RH, we can show (using the same contour idea, but we now have a much larger zero-free region)
\[
\psi(x) = x+O(x^{1/2}(\log x)^2)
\]
This will be an exercise in sheet 3.

Using partial summation, we can deduce that
\[
\pi(x) = Li(x) + O_\varepsilon(x^{1/2}+\varepsilon)
\]
where $Li(x)$ is the logarithmic integral function,
\[
Li(x) &= \int_2^x \frac{1}{\log t} dt\\
&= \frac{x}{\log x} + O(\frac{x}{(\log x)^2})
\]
so
\[
\pi(x) = \frac{x}{\log x} + E(x)
\]
then $E(x) \gg \frac{x}{(\log x)^2}$.

\subsection{Error terms}
In this section, we'll show that, 'often',
\[
|\psi(x)-x| \gg x^{1/2}
\]
Actually we will show that
\[
\psi(x) = x+\Omega_\pm (x^{1/2})
\]
i.e. 
\[
\limsup_{x \to \infty} (\frac{\psi(x)-x}{x^{1/2}}) \geq c > 0
\]
and
\[
\liminf_{x \to \infty} (\frac{\psi(x)-x}{x^{1/2}}) \leq -c < 0
\]

---Lecture 16---

Let's remind you that our goal is to show that $\psi(x) = x+\Omega_\pm (x^{1/2})$.

For contradiction, suppose that $\psi(x)-x \leq cx^{1/2}$ for all large $x$. So $cx^{1/2} - \psi(x) + x \geq 0$ -- take Mellin transform of this.

\begin{lemma} (Landau)\\
Let $A(x)$ be integrable and bounded on any finite interval, and $A(x) \geq 0$ for all $x \geq X$. Let 
\[
\sigma_c = \inf\{\sigma:\int_x^\infty A(x) x^{-\sigma} dx < \infty\}
\]
Then if
\[
F(s) = \int_1^\infty A(x) x^{-s} dx
\]
then $F$ is analytic for $\Re s > \sigma_c$ but \emph{not} at $s=\sigma_c$.
\begin{proof}
Divide integral into $[1,X]$ and $(X,\infty)$, correspondingly partition of $F=F_1+F_2$. $F_1$ is entire (it's a finite integral of an integrable function). If $\Re s > \sigma_c$, the integral converges absolutely, so $F_2$ is analytic for $\Re s > \sigma_c$.\\
By contradiction, suppose $F_2$ is analytic at $s=\sigma_c$.\\
Write $F_2$ as a Taylor series around $\sigma_c+1$,
\[
F_2(s) = \sum_{k=0}^\infty c_k(s-\sigma_c-1)^k
\]
where 
\[
c_k = \frac{F_2^{(k)}(\sigma_c+1)}{k!} = \frac{1}{k!}\int_x^\infty A(x) (-\log x)^k x^{-\sigma_c-1}
\]
This power series has a radius of convergence, which must be $1+\delta$ for some $\delta>0$.\\
Evaluate the series at $\delta=\sigma_c-\frac{\delta}{2}$,
\[
F_2(s) = \sum_{k=0}^\infty \frac{(1-\sigma_c-s)^k}{k!}\int_x^\infty A(x)(\log x)^k x^{-1-\sigma_c} dx
\]
At $s=\sigma_c-\frac{\delta}{2}$, we can interchange integral and summation, so
\[
F_2(\sigma_c-\frac{\delta}{2}) &= \int_x^\infty A(x) x^{-1-\sigma_c} \exp((1+\sigma_c-s)\log x) dx\\
&= \int_x^\infty A(x) x^{-s} dx
\]
so $\int_x^\infty$ converges at $\sigma_c-\frac{\delta}{2}$, contradicting the definition of $\sigma_c$.
\end{proof}
\end{lemma}

\begin{thm} (Landau)\\
If $\sigma_0$ is the supremum of the real parts of $\{\rho: \zeta(\rho) = 0\}$, then\\
(1) For any $\sigma < \sigma_0$, $\psi(x)-x = \Omega_\pm(x^{\sigma})$;\\
(2) If there is a zero $\rho$ with $\sigma=\sigma_0$, then $\psi(x)-x = \Omega_\pm(x^{\sigma_0})$.
\begin{coro}
Assuming there's a zero with $\sigma=\frac{1}{2}$, $\psi(x) - x = \Omega_{\pm}(x^{1/2})$.
\end{coro}
\begin{coro}
RH is \emph{equivalent} to $\psi(x) = x+O(x^{1/2+o(1)})$.
\end{coro}
\begin{proof}
Let $c>0$ be chosen later, and suppose that $\psi(x) - x \leq cx^\sigma$ for all $x \geq X$ (for contradiction). Consider ($\sigma \leq \sigma_0$) 
\[
F(s) = \int_1^\infty (cx^\sigma - \psi(x)+x) x^{-s-1} dx
\]
Recall, by partial summation, that
\[
\frac{\zeta'}{\zeta}(s) = -s\int_1^\infty \psi(x) x^{-s-1} dx
\]
and
\[
\int_1^\infty x^{-s} dx = \frac{1}{s-1}
\]
both for $\Re s > 1$. So $F(s) = \frac{c}{s-\sigma}+\frac{\zeta'(s)}{s\zeta(s)}+\frac{1}{s-1}$ ($\Re s > 1$).\\
This has a pole at $s=\sigma$; it doesn't have a pole at $s=1$ because the contribution from the second and last term cancel each other out, and is analytic for $\Re s > \sigma$.\\
So by Landau's lemma, in fact this integral converges for all $s$ with $\Re s > \sigma$.\\
This proves (1) because if $\sigma < \sigma_0$, then there is a zero of $\zeta$ with $\sigma < \Re \rho \leq \sigma_0$, and at $\rho$, $F$ has a singularity ($\rho \not\in \R)$.\\
Suppose there is $\rho = \sigma_0+it_0$. Repeat the above argument with $\sigma=\sigma_0$, but consider instead
\[
G(s) = F(s) + \frac{e^{i\theta} F(s+it_0) + e^{-i\theta} F(s-it_0)}{2}
\]
where $\theta \in \R$ is chosen later; $G(s)$ is still analytic for $\Re s > \sigma$, and has a pole at $s=\sigma_0$. From $F(s)$, we have residue $c$; from $F(s+it_0)$, we have residue $\frac{m}{\rho}$ where $M$ is the order of $\rho$; from $F(s-it_0)$, we have residue $\frac{m}{\bar{\rho}}$.\\
So $G(s)$ has a pole at $s=\sigma_)$ with residue
\[
c+\frac{e^{i\theta}m}{2\rho} + \frac{e^{-i\theta}m}{2\bar{\rho}} = c-\frac{m}{|\rho|}
\]
where we choose $\theta$ s.t. 
\[
\frac{e^{i\theta}}{\rho} = -\frac{1}{|\rho|}
\]
Now if we choose $c<\frac{m}{|\rho|}$, then this residue is negative.\\
So as $s \to \sigma_0$ from the right, along $\R$, $G(s) \to -\infty$.\\
But for $\Re s > \sigma_0$,
\[
\Re(s) = \int_1^\infty (cx^{\sigma_0}-\psi(x)+x)x^{-s-1}\left(\underbrace{1+\frac{e^{i\theta}x^{-it_0}}{2} + \frac{e^{i\theta}x^{it_0}}{2}}_{=1+\cos(\theta-t_0\log x) \geq 0}\right) dx
\]
So
\[
G(s) = \underbrace{G_1(s)}_{\int_1^x, \text{ entire}} + \underbrace{G_2(s)}_{\int_x^\infty (\geq 0), s \in \R, \Re s > \sigma_0}
\]
cannot go to $-\infty$. Contradiction.\\
This proves $\psi(x)-x = \Omega_+(x^\sigma)$. $\Omega_-$ is the same (multiply by $-1$).
\end{proof}
\end{thm}

\subsection{Functional equation}

---Lecture 17---

The lecture on next Tuesday and Thursday will be given by Jack Throne instead.

Recall that for $\sigma > 0$, $\zeta(s) = 1+\frac{1}{s-1} - s\int_1^\infty \frac{\{t\}}{t^{s+1}} dt$.

We'll try to extend it to other parts of the complex plane.

Let $f(t) = \frac{1}{2}-\{t\}$. Then
\[
\zeta(s) = \frac{1}{s-1} + \frac{1}{2} + s\int_1^\infty \frac{f(t)}{t^{s+1}} dt
\]
But this integral now converges when $\sigma > -1$: let
\[
F(x) = \int_0^x f(t) dt
\]
So we know using integration by part,
\[
\int_X^Y \frac{f(t)}{t^{s+1}} dt = \left[\frac{F(t)}{t^{s+1}}\right]_X^Y + (s+1) \int_X^Y \frac{F(t)}{t^{s+2}} dt
\]
And note that $F(t)$ is \emph{bounded} (consider what $f(t)$ looks like). Therefore, $\int_1^\infty \frac{f(t)}{t^{s+1}} dt$ converges when $\sigma > -1$.

We can now start calculating values of $\zeta$; for example,
\[
\zeta(0) = -\frac{1}{2} \stackrel{?}{=} 1+1+1+...
\]

We can surely continue doing integration by part in this way to define $\zeta$ for $\sigma>-2$, $\sigma>-3$, etc., but there's a better way.

Note that for $-1<\sigma<0$,
\[
s\int_0^1 \frac{f(t)}{t^{s+1}} = \frac{s}{2} \int_0^1 \frac{1}{t^{s+1}} dt - s \int_0^1 \frac{1}{t^s} dt = \frac{1}{2} + \frac{1}{s-1}
\]
so for $-1 < \sigma < 0$, we actually know
\[
\zeta(s) = s\int_0^\infty \frac{f(t)}{t^{s+1}} dt
\]
as a nice single integral.

By Fourier analysis (you can take this for granted if you haven't learnt anything on that; see part IB Methods), as $f$ is periodic, $f(t)$ has a Fourier series
\[
f(t) = \sum_{n=1}^\infty \frac{\sin(2n\pi t)}{n\pi}
\]
which converges for all $t \not\in \Z$. So where $-1<\sigma<0$,
\[
\zeta(s) &= s\int_0^\infty \frac{1}{t^{s+1}} \sum_{n=1}^\infty \frac{\sin(2n\pi t)}{n\pi} dt\\
&=s \sum_{n=1}^\infty \frac{1}{n\pi} \int_0^\infty \frac{\sin(2n\pi t)}{t^{s+1}} dt \text{ (assume we can swap $\int$ and $\sum$ for now)}\\
&= s\sum_{n=1}^\infty \frac{(2n\pi)^s}{n\pi} \int_0^\infty \frac{\sin y}{y^{s+1}} dy \text{ where }y = 2n\pi t
\]
The integral is now independent from $n$, so we can deal with it separately.\\
Here the series 
\[
\sum_{n=1}^\infty \frac{(2n\pi)^s}{n\pi} = 2^s \pi^{s-1}\zeta(1-s)
\]
and 
\[
\int_0^\infty \frac{\sin(y)}{y^{s+1}} dy &= \frac{1}{2i}\left(\int_0^\infty \frac{e^{iy}}{y^{s+1}} dy - \int_0^\infty \frac{e^{-iy}}{y^{s+1}} dy\right)
\]
apply substitution $u=iy$ and $u=-iy$ separately in the two integrals,
\[
=-\sin(\frac{s\pi}{2})\Gamma(-s)
\]
where
\[
\Gamma(s) = \int_0^\infty t^{s-1} e^{-t} dt
\]
is the gamma function, which converges for $\sigma>0$.\\
Let's study the gamma function for a while: use integration by part,
\[
\Gamma(s+1) &= \int_0^\infty t^s e^{-t} dt\\
&= \left[-t^s e^{-t}\right]_0^\infty + s\int_0^\infty t^{s-1} e^{-t} dt\\
&= s\Gamma(s)
\]
In particular, since $\Gamma(1) = \int_0^\infty e^{-t} dt =1$, we know $\Gamma(n) = (n-1)!$ for $n \in \Z$.

So we see that gamma function is a very natural analytic extension to factorial.

Also note that $\Gamma(s+1) = s\Gamma(s)$ allows us to extend $\Gamma(s)$ to the complex plane, with poles at $s=0,-1,-2,...$.

For the zeta function, this means for $-1<\sigma<0$,
\[
\zeta(s) &= s 2^s \pi^{s-1} \zeta(1-s) (-\sin(\frac{s\pi}{2}) \Gamma(-s))\\
&= 2^s \pi^{s-1} \sin(\frac{\pi s}{2}) \Gamma(1-s) \zeta(1-s)
\]
RHS is defined for all $\sigma<0$, so we just define $\zeta(s)$ by above for $\sigma<0$. This gives an analytic continuation of $\zeta(s)$ to $\C$.

\begin{thm} (Functional equation)\\
For \emph{all} $s \in \C$,
\[
\zeta(s) = 2^s \pi^{s-1} \sin(\frac{\pi s}{2}) \Gamma(1-s)\zeta(1-s)
\]
\end{thm}

Note that we know there's pole at $s=1$. How does that fit into this equation? We can try to evaluate $\zeta(1)$ by the above formula:
\[
\zeta(1) = 2 \times 1 \times 1 \times \Gamma(0) \times \zeta(0)
\]
but apparently $\Gamma$ has a pole at $0$, so we haven't broken anything.

Are there any other poles? We have
\[
\zeta(s) = \text{(entire)} \cdot \underbrace{\Gamma(1-s) \zeta(1-s)}_{\text{entire for }\sigma < 0}
\]
So $\zeta(s)$ has no pole for $\sigma<0$. This means that $\zeta(s)$ is analytic everywhere in $\C$ except for a simple pole at $s=1$.

We can also try to evaluate things like
\[
\zeta(2) = 4 \times \pi \times 0 \times \Gamma(-1) \zeta(-1)
\]
We know $\zeta(2) = \frac{\pi^2}{6}$, while $\Gamma$ has a pole at $-1$; but that is cancelled by the $0$ in RHS.\\
We can try the other way:
\[
\zeta(-1) &= \frac{1}{2} \times \frac{1}{\pi^2} \times 1 \times \Gamma(2) \zeta(2)\\
&= \frac{-1}{2\pi^2} \times \frac{\pi^2}{6}\\
&= -\frac{1}{12} \stackrel{?}{=} 1+2+3+4+...
\]
So if you really want to assign a value to this series like a physicists, then this is the value (but don't do it).

What about zeros of $\zeta$? 
\[
0=\zeta(s) = (\neq 0) \cdot \sin(\frac{\pi s}{2}) \Gamma(1-s) \zeta(1-s)
\]
if $\sigma<0$, $\zeta(1-s) \neq 0$, and $\Gamma(1-s) \neq 0$. So the only possibility is that $\sin(\frac{\pi s}{2})$ vanishes, and we know it does at $s=-2n$ for $n \in \N$. So in $\sigma<0$, $\zeta(s)$ has zeros exactly at $-2,-4,-6,...$.

In $\sigma\geq 1$, $\zeta(s)$ has no zeros. So we also know by the functional equation that it has no zeros at $\sigma=0$.

So except for the trivial zeros, $\zeta(s)$ only has zeros in $0<\sigma<1$.

If $0<\sigma<1$, apply the functional equation:
\[
0 = \zeta(s) = (\neq 0) \cdot \Gamma(1-s) \zeta(1-s)
\]
But $\Gamma$ never vanishes in that region. So the zeros of $\zeta$ are symmetric about the point $s=1$. But also $\zeta(\bar{s}) = \bar{\zeta(s)}$, so we get four zeros if $\sigma \neq \frac{1}{2}$, i.e. \emph{wild zeros have to come in fours}. In contrast, if there's a zero on $\sigma=\frac{1}{2}$ then there are only one counterpart (reflection against the real axis). So this is probably why Riemann orignally proposed his hypothesis.

\begin{thm}
RH is equivalent to $\psi(x) = x+O(x^{1/2+o(1)})$.
\begin{proof}
$\implies$ is just by contour integration.\\
For $\Leftarrow$, $\sigma_0=\sup\{\Re \rho: \zeta(\rho) = 0\}$, then $\psi(x) = x+\Omega_\pm(x^\sigma) \forall \sigma<\sigma_0$.\\
So if RH fails, there must be $0<\sigma<1$ s.t. $\sigma \neq \frac{1}{2}$. Therefore by our previous symmetry property, $\sigma_0 > \frac{1}{2}$. So $\psi(x) = x+\Omega_\pm(x^{\sigma'})$ where $\frac{1}{2} < \sigma' < \sigma$; contradiction.
\end{proof}
\end{thm}

\newpage

\section{Primes in arithmetic progressions}

---Lecture 18---

As you probably know, Thomas Bloom is away for a week, so I'll be lecturing this course for today and Thursday. I'll be starting a new chapter today on primes in arithmetic progressions, where in the end we'll prove the Dirichlet theorem (every arithmetic progression contains infinitely many primes), which you probably already know.

\subsection{Dirichlet characters and $L$-functions}

Fix $q \in \N$. A \emph{Dirichlet character of modulus $q$} is a homomorphism $\chi: (\Z/q\Z)^* \to \C^*$. We know $(\Z/q\Z)^*$ is a finite abelian group of order $\phi(q)$, so the set of Dirichlet characters modulus $q$ form a finite abelian group of the same order by the obvious definition of multiplication.

We can also think of the Dirichlet character $\chi$ as defining a function $\chi:\Z \to \C$, given by the formula
\[
\chi(a) = \left\{
\begin{array}{ll}
\chi(a\ mod\ q) & (a,q)=1\\
0 & (a,q)>1
\end{array}
\right.
\]
Note this $\chi$ is periodic with period $q$, and is also \emph{totally} multiplicative.\\
If $\chi$ is the trivial homomorphism on $(\Z/q\Z)^*$, we call it the principal (Dirichlet) character of modulus $q$, usually denoted as $\chi_0$.

We'll first justify why this function is called a \emph{character}: in fact, it is just a one dimensional character of the group $(\Z/q\Z)^*$ (see part II Representation Theory).
\begin{lemma}
(1) Let $\chi$ be a Dirichlet character of modulus $q$. Then
\[
\sum_{a \in (\Z/q\Z)^*} \chi(a) = \sum_{1 \leq a \leq q} \chi(a) = \left\{
\begin{array}{ll}
\phi(q) & \chi=\chi_0\\
0 & \chi \neq \chi_0
\end{array}
\right.
\]
(2) Let $a \in (\Z/q\Z)^*$. Then
\[
\sum_\chi \chi(a) = \left\{\begin{array}{ll}
\phi(q) & a \equiv 1 \pmod q\\
0 & a \not\equiv1 \pmod q
\end{array}
\right.
\]
\begin{proof}
We treat (2): If $a \equiv 1 \pmod q$, then $\chi(a) = 1$ for all $\chi$. So $\sum_\chi \chi(a) = \sum_\chi 1 = \phi(q)$.\\
If $a \not\equiv 1 \pmod q$, then there exists $\psi:(\Z/q\Z)^* \to \C^*$ such that $\psi(a) \neq 1$. The map $\chi \to \chi \psi$ is a permutation of the set of Dirichlet characters mod $q$, hence
\[
\sum_\chi \chi(a) = \sum_\chi (\chi\psi)(a) = \psi(a) \sum_\chi \chi(a)
\]
$\psi(a) \neq 1$, so we must have $\sum_\chi \chi(a) = 0$.

\end{proof}
\end{lemma}

Let $1_{x \equiv a \pmod q}:\Z \to \C$ be defined by 
\[
1(x) = \left\{\begin{array}{ll} 
1 & x \equiv a \pmod q\\
0 & else
\end{array}
\right.
\]
where $a \in \Z, (a,q) = 1$.\\
Then the above lemma (2) says\footnote{The summand in RHS is $\chi(x/a)$ as $\chi$ is a homomorphism, then compare with the lemma above.}
\[
1_{x \equiv a \pmod q} (x) = \frac{1}{\phi(q)} \sum_\chi \chi(a)^{-1} \chi(x)
\]
It follows that
\[
\sum_{p \leq x,p \equiv a \pmod q} 1 &= \sum_{p \leq x} 1_{x \equiv a \pmod q}(p)\\
&= \frac{1}{\phi(q)} \sum_{p \leq x} \sum_\chi \chi(a)^{-1} \chi(p)
\]
Estimating this is closely related to estimating, for example, 
\[
\frac{1}{\phi(q)} \sum_{n \leq x} \sum_\chi \chi(a)^{-1} \chi(n) \Lambda(n) &= \sum_\chi \frac{\chi(a)^{-1}}{\phi(q)} \sum_{n \leq x} \chi(n) \Lambda(n)
\]
So the strategy to prove Dirichlet's theorem is to consider the contribution of each character $\chi$ separately.

We'll do this using the Dirichlet $L$-functions 
\[
L(s,\chi) = \sum_{n \geq 1} \chi(n)n^{-s}
\]
This series converges absolutely in the region $\sigma>1$ (as $\chi(n)$ must be some roots of unity), and therefore defines an analytic function there.

\begin{lemma}
If $\chi \neq \chi_0$, then $\sum_{n \geq 1} \chi(n) n^{-s}$ converges in $\sigma>0$.
\begin{proof}
Use partial summation with the obvious choice $a_n = \chi(n)$, $A(x) = \sum_{n \leq x} \chi(n)$, $f(t) = t^{-s}$,
\[
\sum_{n \leq x} \chi(n) n^{-s} &= A(x) x^{-s} - \int_{t=1}^x A(t) f'(t) dt 
\]
Now lemma says $\sum_{1 \leq n \leq q} \chi(n) = 0$, as $\chi \neq \chi_0$. Hence $A(n)$ is periodic, and $|A(x)| \leq \phi(q)$ for all $x$. So $|A(x)x^{-s}| \leq \phi(q) x^{-\sigma}$ and the integral is absolutely convergent.
\end{proof}
\end{lemma}

Consequence: $L(s,\chi)$ is analytic in the same region $\sigma>0$, and in particular does not have a pole at $s=1$.\\
Since $\chi(n)$ is multiplicative, we have an Euler product identity:
\[
L(s,\chi) = \prod_p (1-\chi(p) p^{-s})^{-1}
\]
which is valid in the region $\sigma>1$.\\
Consequence 1: when $\chi = \chi_0$, $L(s,\chi_0) = \zeta(s) \prod_{p | q} (1-p^{-s})$. Hence $L(s,\chi_0)$ has a meromorphic continuation to all $s \in \C$ and a simple pole at $s=1$.\\
Consequence 2: In the region $\sigma > 1$, and for any $\chi$, we have a formula for the logarithmic derivative of $L(s,\chi)$ in the same way as for $\zeta(s)$.

This gives an identity 
\[
-\frac{L'}{L}(s,\chi) = \sum_{n \geq 1} \chi(n) \Lambda(n) n^{-s}
\]

---Lecture 19---

Last time we fixed $q \in \N$ and introduced the Dirichlet $L$ function: for any Dirichlet character $\chi:(\Z/q\Z)^* \to \C^*$,
\[
L(s,\chi) = \sum_{n \geq 1} \chi(n) n^{-s}
\]
where we abuse the notation to extend $\chi$ to a function $\N \to \C$.\\
We observed that this was absolutely convergent for $\sigma>1$. We also observed that when $\chi \neq \chi_0$ (the trivial character), $L(s,\chi)$ is convergent in $\sigma>0$.\\
We also had a formula for the log derivative:
\[
\log L(s,\chi) = \sum_p \sum_{k \geq 1} \chi(p)^k p^{-ks}/k
\]
hence
\[
\frac{L'(s,\chi)}{L(s,\chi)} &= \sum_p \sum_{k \geq 1} \chi(p)^k (-\log p) p^{-ks}\\
&= -\sum_{n \geq 1} \chi(n)\Lambda(n) n^{-s}
\]
which is valid in $\sigma>1$.

Fix $a \in \N$, $(a,q) = 1$. We combine this with the identity valid for any natural number $n$:
\[
1_{n \equiv a \pmod q}(n) = \frac{1}{\phi(q)}\sum_\chi \chi(a)^{-1} \chi(n)
\]
We get
\[
\sum_{n \geq 1} 1_{n \equiv a \pmod q} (n) \Lambda(n)n^{-s} = -\frac{1}{\phi(q)} \sum_\chi \chi(a)^{-1} \frac{L'(s,\chi)}{L(s,\chi)}
\]
which is valid in $\sigma>1$.

\subsection{Dirichlet's theorem}

\begin{thm}
Let $a \in \N$, $(a,q)=1$. There are infinitely many primes $p$ s.t. 
\[
p \equiv a \pmod q
\]
How are we going to prove it? We have $L(s,\chi_0) = \zeta(s) \prod_{p|q} (1-p^{-s})$, so as we saw last time, $L(s,\chi_0)$ has a simple pole at $s=1$. Hence
\[
\sum_{n \geq 1} 1_{n \equiv a \pmod q}(n)\Lambda(n) n^{-s} &= \frac{1}{\phi(q)} \frac{1}{s-1} + O(1) + \sum_{\chi \neq \chi_0} \chi(a)^{-1}\frac{L'(s,\chi)}{L(s,\chi)}
\]
Let's assume that we understand that the last term is bounded. Then RHS (and so LHS) is divergent at $s=1$. But if there are finitely many primes $p \equiv a \pmod q$, LHS would be bounded as $s \to 1$. So to show Dirichlet's theorem it's enough to show that $\forall \chi \neq \chi_0$, $\frac{L'(s,\chi)}{L(s,\chi)}$ is bounded, or analytic at $s=1$ since we know it's meromorphic.

This is equivalent to showing that if $\chi \neq \chi_0$, then $L(1,\chi) \neq 0$.
\end{thm}

\begin{thm}
If $\chi \neq \chi_0$, then $L(1,\chi) \neq 0$.
\begin{proof}
\[
\prod_\chi L(s,\chi) &= \exp(\sum_\chi \log L(s,\chi))\\
&= \exp(\sum_\chi \sum_p \sum_{k \geq 1} \chi(p)^k p^{-ks}/k)\\
&= \exp(\sum_\chi \sum_{n \geq 1} \chi(n) \frac{n^{-s} \Lambda(n)}{\log n})\\
&= \exp\left(\sum_{n \geq 1}\left[\frac{n^{-s} \Lambda(n)}{\log n} \sum_\chi \chi(n)\right]\right)
\]
We know by a previous lemma that
\[
\chi(n) = \left\{\begin{array}{ll}
0 & (q,n)>1\ or\ (q,n)=1, n \not\equiv 1\pmod q\\
\phi(q) & n \equiv 1 \pmod q
\end{array}
\right.
\]
Hence
\[
\prod_\chi L(s,\chi) = \exp\left(\sum_{n \geq 1, n \equiv 1 \pmod q} \frac{n^{-s}\Lambda(n)}{\log n} \phi(q)\right)
\]
valid in $\sigma>1$. The exponent is a non-negative real number for $s>1$ real.

So for $s \in (1,\infty)$, $\prod_\chi L(s,\chi) \in [1,\infty)$.

Note that $L(s,\chi_0)$ has a simple pole at $s=1$. If there are at least two dinstinct characters $\psi,\psi'$ of modulus $q$ such that $L(1,\psi) = L(1,\psi') = 0$, then $\prod_\chi L(s,\chi)$ would be analytic in a neighbourhood of $s=1$, and vanish at $s=1$; but this is impossible, so there's at most one character $\psi$ s.t. $L(1,\psi)=0$.\\
Note: for any $\chi$, $L(1,\bar{\chi}) = \overline{L(1,\chi)}$. So if $L(1,\chi) = 0$, then $L(1,\bar{\chi}) = 0$.\\
Hence if $L(1,\chi) = 0$, then $\chi = \bar{\chi}$. In other words (as $|\chi$ must have modulus 1), $\chi$ takes values in $\{\pm 1\}$ (we call such characters quadratic as then $\chi^2=1$).\\
Suppose for contradiction that there exists a non-principal but quadratic character $\psi: (\Z/q\Z)^* \to \{\pm 1\}$ s.t. $L(1,\psi) = 0$.\\
We consider the product $L(s,\psi)\zeta(s)$. This function is analytic in $\sigma>0$.\\
In $\sigma>1$, we have the expression
\[
L(s,\psi) \zeta(s) = (\sum_{n \geq 1} \psi(n) n^{-s})(\sum_{n \geq 1} n^{-s}) = \sum_{n \geq 1} r(n) n^{-s}
\]
where $r(n) = \sum_{d | n} \psi(d)$. First of all, $r$ is multiplicative; also $r(n) \geq 0$: we need to only prove it for prime powers:
\[
r(p^k) = \psi(1)+\psi(p) + ... + \psi(p^k) = \left\{\begin{array}{ll}
k+1 & \psi(p) = 1\\
1 & \psi(p) = 0\ or \ \psi(p) = -1, k \text{ is even}\\
0 & \psi(p) = -1, k \text{ is odd}
\end{array}
\right.
\]
We also know $r(n^2) \geq 1$ by the same argument.

We now use Landau's lemma:
\begin{lemma}
Let $f(s) = \sum_{n \geq 1} a_n n^{-s}$ where $a_n$ are non-negative real numbers. Suppose given $\sigma_0$ s.t. $f(s)$ is convergent, therefore absolutely convergent in $\sigma > \sigma_0$. Suppose as well that $f(s)$ admits an analytic continuation to the disc $\{|s-\sigma_0| < \varepsilon\}$. Then $f(s)$ is convergent in the region $\sigma>\sigma_0-\varepsilon$.
\end{lemma}

Let $f(s) = L(\psi,s) \zeta(s) = \sum_{n \geq 1} r(n) n^{-s}$, valid in $\sigma>1$. Then we can use Landau's lemma, together with the fact that $f(s)$ is analytic in $\sigma>0$ to conclude that $f(s)$ is convergent in $\sigma>0$.\\
But this can't be true: 
\[
f(1/2) = \sum_{n \geq 1} r(n) n^{-1/2} \geq \sum_{n \geq 1} r(n^2)/n \geq \sum_{n \geq 1} 1/n = \infty
\]
This is impossible. So $L(1,\psi) \neq 0$.
\end{proof}
\end{thm}

\subsection{Zero-free region}

---Lecture 20 missed, but lecturer wasn't able to make it anyway---

---So still Lecture 20---

Example class today at 330pm MR14!

We've proved that there are infinitely many primes $\equiv a pmod q$ where $(a,q)=1$ (Dirichlet's theorem). We did this using
\[
-\frac{L'}{L}(s,\chi) = \sum_{n=1}^\infty \frac{\Lambda(n)\chi(n)}{n^s}
\]

We'd like to prove some sort of PNT for such primes. To do this, we'll use Perran's formula in a similar fashion that we did to $\zeta$ function. 

We need information about the zeros of $L(s,\chi)$. We saw last week that $L(1,\chi) \neq 0$.

Similarities to zero-free region for $\zeta(s)$, but important difference: $\zeta(s)$ has a pole at $s=1$, but $L(s,\chi)$ has no poles for $\sigma>0$ if $\chi \neq \chi_0$. For $zeta$ we had
\[
\zeta(s) = 1+\frac{1}{s-1} - s\int\frac{\{t\}}{t^{s+1}} dt
\]
so we can show $\zeta(\sigma) \neq 0$ when $0 < \zeta < 1$. But this is a problem for $L$-function.

Let $\tau = |t|+4$ (basically for us to always be able to write $\log \tau$).

Recall the following lemma:
\begin{lemma}
If $f(z)$ is analytic on a region containing $|z| \leq 1$ and $f(0) \neq 0$, and $|f(z)| \leq M$ (bounded) for $|z| \leq 1$. Then for $0 < r < R < 1$, for $|z| \leq r$, we have
\[
\frac{f'}{f}(z) = \sum \frac{1}{z-z_k} + O(\log \frac{M}{|f(0)|})
\]
where $z_k$ is the zeros of $f$ inside the disk $|z_k| \leq R$.
\end{lemma}
We can use this to get a nice formula for $L$-function:

\begin{lemma}
If $\chi \neq \chi_0$ and $\frac{5}{6} \leq \sigma \leq 2$, then
\[
\frac{L'}{L}(s,\chi) = \sum_\rho \frac{1}{s-\rho}+O(\log q \tau)
\]
over $\rho$ all the zeros with $|\rho-(3/2+it)| \leq 5/6$.
\begin{proof}
This basically follows from the previous lemma by setting $f(z) = L(z+3/2+it,\chi)$, $R=5/6$, $r=2/3$. Note that
\[
|f(0)| &= |L(3/2+it,\chi)|\\
&= \prod_p |1-\frac{\chi(p)}{p^{3/2+it}}|\\
&\geq \prod_p (1-\frac{1}{p^{3/2}})^{-1} \gg 1
\]
By partial summation, if $F(t) = \sum_{1 \leq n \leq t} \chi(n)$, for $\sigma>0$,
\[
L(s,\chi) = s\int_1^\infty \frac{F(t)}{t^{s+1}}dt
\]
so
\[
|L(s,\chi)| \ll |s|q \int_1^\infty \frac{1}{t^{\sigma+1}}dt \ll q\tau
\]
\end{proof}
\end{lemma}

\begin{thm}
Let $\chi$ be a character. There is an absolute constant $c>0$ s.t.
\[
L(s,\chi) \neq 0
\] 
if $\sigma>1-\frac{c}{\log(q\tau)}$.\\
It would be very nice if we could prove this, but unfortunately we couldn't do that for general characters. We can only prove this for $\chi$ a non-quadratic character ($\chi(n) \in \R$ $\forall n$ and $\chi \neq \chi_0$).
\begin{proof}
If $\chi=\chi_0$, since $L(s,\chi_0) = \zeta(s) \prod_{p | q} (1-p^{-s})$, in this region $\sigma>0$, zeros of $L(s,\chi_0)$ are the same as $\zeta(s)$, so done (in fact we proved a better result for $\zeta$).\\
Let $\rho = \sigma+it$ s.t. $L(\rho,\chi) = 0$. The idea is to compare ($\delta \to 0^+$)
\[
\frac{L'}{L}(1+\delta+it,\chi),\frac{L'}{L}(1+\delta+2it,\chi^2), \frac{L'}{L}(1+\delta,\chi_0)
\]
Use the same trick as in $\zeta$ function, consider
\[
&\Re(-3\frac{L'}{L}(1+\delta,\chi_0) - 4\frac{L'}{L}(1+\delta+it,\chi)-\frac{L'}{L}(1+\delta+2it,\chi^2))\\
=&\sum_{n=1,(n,q)=1}^\infty \frac{\Lambda(n)}{n^{1+\delta}} \Re(3+4\chi(n)n^{-it}+\chi(n)^2 n^{-2it})
\]
and we'll again use the fact that $\forall \theta$ $3+4\cos\theta+\cos(2\theta) \geq 0$, i.e. $\Re(3+4e^{i\theta}+e^{i2\theta})$.

By the lemma,
\[
-\Re \frac{L'}{L}(1+\delta,\chi_0) &= \frac{1}{\delta} + O(\log q),\\
-\Re \frac{L'}{L}(1+\delta+it,\chi) &\leq -\frac{1}{1+\delta-\sigma}+O(\log q\tau),\\
-\Re \frac{L'}{L}(1+\delta+2it,\chi^2) &\ll \log(q\tau)
\]
(since $\chi^2 \neq \chi_0$ by assumption that it's not a quadratic character, so there's no pole at 1). Now we can combine this and get
\[
\frac{3}{\delta} - \frac{4}{1+\delta-\tau} + O(\log \tau) \geq 0
\]
contradiction if we choose $\delta \approx \frac{c'}{\log q\tau}$ and $\sigma \geq 1-\frac{c}{\log q\tau}$.
\end{proof}
\end{thm}

\begin{thm}
If $\chi$ is a quadratic character, $\exists c > 0$ s.t.
\[
L(s,\chi) \neq 0 \text{ if } \sigma>1-\frac{c}{\log q\tau} \text{ and } t \neq 0
\]
So we have a zero-free region provided that we're not on the real line.\\
\end{thm}
But we cannot rule out a zero $\rho$ of $L(s,\chi)$ with $\rho \in \R$ close to 1 (this is still open). We had a weaker result:
\begin{thm}
Let $\chi$ be a quadratic character. There is an absolute constant $c > 0$ s.t. $L(s,\chi)$ has at most \emph{one} zero $\rho \in (0,1)$ s.t. $\rho \geq 1-\frac{c}{\log q}$.\\
These zeros are called the \emph{exceptional zeros}, if they exist (but hopefully they don't).
\end{thm}

To prove the above two theorems, first we need a lemma for $L(s,\chi_0)$.
\begin{lemma}
If $5/6 \leq \sigma \leq 2$, then
\[
-\frac{L'}{L}(s,\chi_0) = \frac{1}{s-1} - \sum_\rho \frac{1}{s-\rho} + O(\log q\tau)
\]
over zeros $\rho$ with $|\rho-(3/2+it)| \leq 5/6$.
\begin{proof}
This essentially follows from the same formula for $\zeta$ function:
\[
-\frac{\zeta'}{\zeta}(s) = -\sum_\rho \frac{1}{s-\rho}+O(\log \tau) + \frac{1}{s-1}
\]
since\\
(a) for $\sigma>0$, zeros of $\zeta$ are zeros of $L(s,\chi_0$) (so we're summing up the same set of $\rho$);\\
(b) by the Euler product,
\[
\frac{L'}{L}(s,\chi_0) = \frac{\chi'}{\chi}(s) + \sum_{p|q} \frac{\log p}{p^s-1} \ll \omega(q) \ll \log q
\]
There are only 3 minutes left, so I'll just quickly sketch a proof and do them properly next time.\\
First theorem: for $t$ large, same as previous proof ($\chi^2=\chi_0$, but no pole).\\
For $t$ small, $0 < |t| \ll \frac{1}{\log q\tau}$, instead of comparing $\chi_0$, $\chi$, $\chi^2$ we compare $\rho$ and $\bar{\rho}$.\\
Second theorem: we can no longer do either of the above, but we allowed one zero, so if there are more than one then we'll compare two such real zeros (we'll explain what we mean by \emph{compare} exactly on the next lecture).
\end{proof}
\end{lemma}

---Lecture 21---

Now we're going to prove properly the two theorems above quadratic characters we stated before:
\begin{thm}
Let $\chi$ be a quadratic character. Then there is an absolute constant $c>0$ s.t.
\[
L(s,\chi) \neq 0 \text{ if } \sigma>1 - \frac{c}{\log q\tau} \text{ and } t \neq 0
\]
\begin{proof}
As before, let $\rho = \sigma+it$ be a zero of $L(s,\chi)$. Let $\delta > 0$ which we're going to choose later. By above lemma 1 (on page 56),
\[
&-\frac{L'}{L}(1+\delta+it,\chi) = -\sum_{\rho'} \frac{1}{1+\delta+it-\rho'} + O(\log q \tau)\\
\implies &-\Re \frac{L'}{L}(1+\delta+it,\chi) \leq -\frac{1}{1+\delta+it-\rho} + O(\log q \tau) = -\frac{1}{1+\delta-\sigma} + O(\log q\tau)
\]
and
\[
-\Re\frac{L'}{L}(1+\delta,\chi_0) \leq \frac{1}{\delta} + O(\log q\tau)
\]
the problem now is that $\chi_0$ is the principal character, so we're in the case of lemma 2 (on page 58) instead. First suppose $\tau \geq C(1-\sigma)$:
\[
-\Re \frac{L'}{L}(1+\delta+2it,\chi^2) &= -\Re\frac{L'}{L}(1+\delta+2it,\chi_0)\\
&\leq \Re\frac{1}{\delta+2it} + O(\log q\tau)\\
&\leq \frac{\delta}{\delta^2+4t^2} + O(\log q\tau)
\]
As before,
\[
\Re(-3\frac{L'}{L}(1+\delta,\chi_0) - 4\frac{L'}{L}(1+\delta+it,\lambda) - \frac{L'}{L}(1+\delta+2it,\chi^2)) \geq 0
\]
But on the other hand, it also
\[
\leq \frac{3}{\delta} - \frac{4}{1+\delta-\sigma} + \frac{\delta}{\delta^2+4t^2} + O(\log q\tau)
\]
If $\sigma=1$, we get a contradiction as $\delta \to 0$ (we assumed $t \neq 0$). Otherwise, if we choose $\delta = c(1-\sigma)$, then the above gives
\[
0 \leq \frac{3}{c(1-\sigma)} - \frac{4}{(c+1)(1-\sigma)} + \frac{c'}{1-\sigma} + O(\log q\tau)
\]
We can choose $c, C$, hence $c'$, s.t. the above $\leq -\frac{c''}{1-\sigma}+O(\log q\tau)$ and so $\sigma \leq 1-\frac{c'''}{\log q\tau}$.\\
For small $\tau$, we need a different argument. In particular we're not going to use this mysterious $3-4+...$ argument. In fact there's a simpler argument that will work: since $L(\rho,\chi) = 0$, also $L(\bar{\rho},\chi) = 0$ ($L(s,\chi) = s\int_1^\infty \frac{\sum_{1 \leq n \leq t}\chi(n)}{t^{s+1}} dt$). It follows that
\[
-\Re \frac{L'}{L}(1+\delta+it,\chi) \leq -\Re \frac{1}{1+\delta-\rho}-\Re\frac{1}{1+\delta-\bar{\rho}} + O(\log q\tau)
\]
(assuming that $|t| \leq c(1-\sigma)$, in particular $|t| \leq c'$ for some small constant).\\
RHS is
\[
\frac{-2(1+\delta-\sigma)}{(1+\delta-\sigma)^2 + t^2} + O(\log q\tau)
\]
As before,
\[
-\frac{L'}{L}(1+\delta,\chi_0) \leq \frac{1}{\delta} + O(\log q\tau)
\]
Now
\[
&(-\Re\frac{L'}{L}(1+\delta,\chi_0) - \Re \frac{L'}{L} (1+\delta+it,\chi))\\
&=\sum_{n=1,(n,q) = 1}^\infty \frac{\Lambda(n)}{n^{1+\delta}}(1+\Re( \underbrace{\chi(n)n^{it}}_{|z|=1})) \geq 0
\]
so
\[
\frac{1}{\delta} - \frac{2(1+\delta-\sigma)}{(1+\delta-\sigma)^2+t^2} + O(\log q\tau) \geq 0
\]
Now we're in good shape: if we choose $\delta = c(1-\sigma)$, then LHS is $\leq -\frac{c'}{(1-\sigma)}+O(\log q\tau)$. So $\sigma \leq 1-\frac{c''}{\log q\tau}$.
\end{proof}
\end{thm}

\begin{thm}
Let $\chi$ be a quadratic character. There is $c >0$ s.t. there is at most one real zero $\rho \in (0,1)$ of $L(s,\chi)$ s.t.
\[
\rho \geq 1-\frac{c}{\log q}
\]
\begin{proof}
Suppose $\rho_0 < \rho_1 \leq 1$ are zeros of $L(s,\chi)$. Then for $\sigma \in (0,1)$, by the same argument
\[
-\Re \frac{L'}{L}(\sigma,\chi) &\leq -\Re\frac{1}{\sigma-\rho_0}-\Re\frac{1}{\sigma-\rho_1} + O(\log q)
\]
for $\sigma \geq 1-\frac{1}{1000000}$, say.\\
So 
\[
\frac{1}{\sigma-1}-\frac{2}{\sigma-\rho_0} + O(\log q) \geq (-\Re\frac{L'}{L} (\sigma,\chi_0) - \Re \frac{L'}{L}(\sigma,\chi)) \geq 0
\]
Hence $\rho_0 \leq 1-\frac{c}{\log q}$.
\end{proof}
\end{thm}

Now we have the zero-free region for $L$ function, with about the same strength as for $\zeta$ function, so we can prove an analogue of PNT (which we'll do in next lecture). We'll first introduce a lemma:
\begin{lemma}
If $\chi \neq \chi_0$, and $\sigma \geq 1-\frac{c}{\log q\tau}$ (for some absolute $c>0$, then if \emph{either} $\chi$ has no exceptional zero, \emph{or} $\chi$ has exceptional zero at $\beta$, but $|s-\beta| \geq \frac{1}{\log q}$, then
\[
\frac{L'}{L}(s,\chi) \ll \log q\tau
\]
\begin{proof}
If $\sigma>1$, note
\[
|\frac{L'}{L}(s,\chi)| \leq \sum_{n=1,(n,q)=1}^\infty \frac{\Lambda(n)}{n^\sigma} \ll \frac{1}{\sigma-1}
\]
In particular, if $s=\sigma+it$ and $s_1 = 1+\frac{1}{\log q\tau} + it$, $|\frac{L'}{L}(s,\chi)| \ll \log q\tau$. By lemma 1,
\[
\frac{L'}{L}(s,\chi) = \sum_\rho \frac{1}{s-\rho} + O(\log q\tau)
\]
for all zeros $\rho$, $|s-\rho| \asymp |s_1\rho|$. So
\[
|\frac{L'}{L}(s,\chi) - \frac{L'}{L}(s_1,\chi)| &\ll |\sum_\rho \frac{1}{s-\rho} - \frac{1}{s_1-\rho}| + O(\log q\tau)\\
&\ll \Re \sum_\rho \frac{1}{s_1-\rho} + O(\log q\tau) \ll \log q\tau
\]
\end{proof}
\end{lemma}

\subsection{Prime Number Theorem for Arithmetic Progressions}
Recall that
\[
\sum_{1 \leq n \leq x, n \equiv a \pmod q} \Lambda(n) &= \frac{1}{\varphi(q)} \sum_\chi \overline{\chi(a)} \sum_{1 \leq n \leq x} \Lambda(n) \chi(n)\\
&=\frac{1}{\varphi(q)} \sum_\chi \overline{\chi(a)} \psi(x,\chi)
\]

\begin{thm}
If $q \leq \exp(O(\sqrt{\log x}))$, then
(1)
\[
\psi(x,\chi_0) = x + O(x\exp (-c\sqrt{\log x}))
\]
(2)
If $\chi \neq \chi_0$, $\chi$ has no exceptional zero, then $\psi(x,\chi) = O(x\exp (-c\sqrt{\log x}))$;\\
(3) If $x \neq x_0$, and we have an exceptional zero at $\beta$, then $\psi(x,\chi) = -\frac{x^\beta}{\beta} + O(x \exp(-c \sqrt{\log x}))$.\\
We'll prove this on Saturday.
\end{thm}

---Lecture 22---

addition to section 4.3:

\begin{thm}
If $\chi_1,\chi_2$ are distinct quadratic characters modulo $q$, then $L(s,\chi_1) L(s,\chi_2)$ has at most one real zero $\beta$, $1-\frac{c}{\log q} < \beta < 1$ (sometimes called the exceptional character of $q$).
\begin{proof}
Say $\beta_i$ is a real zero of $L(s,\chi_i)$ ($i=1,2$). WLOG suppose $5/6 \leq \beta_1\leq\beta_2 < 1$ (if they are less than $5/6$ then we're good anyway. Fix $\delta>0$, we have the following:\\
(1) $-\Re\frac{L'}{L} (1+\delta,\chi_i) \leq -\frac{1}{1=\delta-\beta_i}+O(\log q)$ ($i=1,2$);\\
(2) $-\Re\frac{L'}{L}(1+\delta,\chi_1\chi_2) \leq O(\log q)$ (the product is not principal because $\chi_1 \neq \chi_2$;\\
(3) $-\frac{\zeta'}{\zeta}(1+\delta) \leq \frac{1}{\delta} + O(1)$ ($-\frac{\zeta'}{\zeta}(s) = \frac{1}{s-1}+O(1)$) (we could also just stick with $-\Re\frac{L'}{L}(1+\delta,\chi_0)$).

Therefore, consider the following Dirichlet series with non-negative coefficients,
\[
-\frac{\zeta'}{\zeta}(1+\delta) - \frac{L'}{L}(1+\delta,\chi_1)-\frac{L'}{L}(1+\delta,\chi_2) - \frac{L'}{L}(1+\delta,\chi_1\chi_2) \leq \frac{1}{\delta} - \frac{2}{1+\delta-\beta_1} + O(\log q)
\]
so choose $\delta = c(1-\beta_1)$, and therefore $\beta_1 \leq 1-\frac{c}{\log q}$.
\end{proof}
\end{thm}

Back to section 4.4 where we stated a theorem last time:

Let $\psi(x,\chi) = \sum_{n \leq x} \Lambda(n) \chi(n)$.
\begin{thm}
Let $q \leq \exp(O(\sqrt{\log x}))$. Then
\[
\psi(x,\chi_0) = x + O(x \exp(-c\sqrt{\log x}))
\]
If $\chi \neq \chi_0$, we get a cancellation $\psi(x,\chi) = O(x \exp(-c\sqrt{\log x}))$ unless $\chi$ has an exceptional zero.\\
If $\chi$ has an exceptional zero at $\beta$, then in fact we get another main term:
\[
\psi(x,\chi) = -\frac{x^\beta}{\beta}+O(x\exp(-c\sqrt{\log x}))
\]
Recall that
\[
1_{n \equiv a \pmod q} = \frac{1}{\varphi(q)} \sum_\chi \overline{\chi(a)}\chi(n)
\]
we get an expression
\[
\psi(x;q,a) &= \sum_{n \leq x, n \equiv a \pmod q} \Lambda (n)\\
&=\frac{1}{\varphi(q)}\sum_\chi \overline{\chi(a)} \psi(x,\chi)
\]
\begin{coro}
If $(a,q) = 1$, then ($q \leq \exp(O(\sqrt{\log x}))$)
\[
\psi(x;q,a) = \frac{x}{\varphi(q)} + O(x \exp(-c\sqrt{\log x})) \text{ (no exceptional zero)}
\]
If $q$ has an exceptional zero at $\beta$ and $\chi_1$, then
\[
\psi(x;q,a) = \frac{x}{\varphi(q)} - \frac{\chi_1(a)}{\varphi(q)} \frac{x^\beta}{\beta}+O(x \exp(-c\sqrt{\log x}))
\]
\end{coro}
\begin{proof} (of theorem)\\
By Perran's formula ($\sigma_0 > 1, T \geq 1$), 
\[
\psi(x,\chi) = -\frac{1}{2\pi i} \int_{\sigma_0 - iT}^{\sigma_0+iT} \frac{L'}{L}(s,\chi) \frac{x^s}{s}ds + O\left(\frac{x}{T} \sum_{\frac{x}{2} < n < 2x} \frac{\Lambda(n)}{|x-n|} + \frac{x^{\sigma_0}}{T}\sum_{n=1}^\infty \frac{\Lambda(n)}{n^{\sigma_0}}\right)
\]
By the same argument as for $\zeta(s)$, error term is $\ll \frac{x(\log x)^2}{T}$ ($\sigma_0 = 1+\frac{1}{\log x}$).\\
Take $C$ to be a rectangular contour with corners at $\sigma_0 \pm iT, \sigma_1 \pm iT$, we get (integrand omitted)
\[
\psi(x,\chi) = \frac{1}{2\pi i} \int_C +O(\int_{\sigma_1\pm iT} + \int_{\sigma_0+iT}^{\sigma_1+iT} + \int_{\sigma_0-iT}^{\sigma_1-iT} + \frac{x(\log x)^2}{T})
\]
\end{proof}
\end{thm}
Error terms we bound in the same way as for $\zeta(s)$, so in total,
\[
\psi(x,\chi) = -\frac{1}{2\pi i} \int_C \frac{L'}{L}(s,\chi) \frac{x^s}{s} ds + O\left(\underbrace{\frac{x(\log x)^2}{T} + x^{\sigma_1}}_{\ll \exp(-c \sqrt{\log x}),T=\exp(O(\sqrt{\log x}))}\right)
\]
and we choose $\sigma_1 = 1-\frac{c}{\log qT}$ so $x^{\sigma_1} \ll x \exp (-c\sqrt{\log x})$ if $q \ll T \approx \exp(O(\sqrt{\log x}))$.

How about the main term? If $\chi = \chi_0$, take $\sigma_1$ as above, so no zeros of $L(s,\chi_0)$, so $\frac{L'}{L}$ has just a simple pole at $s=1$, so $\frac{1}{2\pi i} \int_C = x$;\\
If $\chi \neq \chi_0$, there's no exceptional zero, no zeros of $L(s,\chi)$ with $\sigma \geq \sigma_1$, so no poles of $\frac{L'}{L}(s,\chi)$ so $\int_C = 0$.\\
If $\chi$ has an exceptional zero at $\beta$, then inside $C$, $\frac{L'}{L}$ has a pole at $\beta$. So $\frac{L'}{L}(s,\chi) \frac{x^s}{s}$ has residue $\frac{x^\beta}{\beta}$ at this pole, so 
\[
\frac{1}{2\pi i} \int_C \frac{L'}{L}(s,\chi) \frac{x^s}{s} ds = \frac{x^\beta}{\beta}
\]

\subsection{Siegel-Walfisz Theorem}

\begin{thm} (1, S-W)\\
$\forall A > 0$, if $q \leq (\log x)^A$, and $(a,q) = 1$, then
\[
\psi(x;q,a) = \frac{x}{\varphi(q)} + O_A(x \exp(-c\sqrt{\log x}))
\]
Some people might call this the PNT for AP because this has not many if and then restrictions (comparing to what we've proved in the previous section) and it has the right main term and error term.
\end{thm}
This follows from
\begin{thm} (2)\\
If $q \leq (\log x)^A$ and $x$ is large enough (depending on $A$), then 
\[
\psi(x,\chi) = O_A(\exp (-c\sqrt{\log x}))
\]
$\forall \chi \neq \chi_0$.
\end{thm}
This in turn follows from the following theorem,
\begin{thm} (3)\\
$\forall \varepsilon > 0$, $\exists C(\varepsilon)$ s.t. if $\chi$ is a quadratic character modulo $q$, and $\beta$ is a real zero, then 
\[
\beta < 1 - c_\varepsilon q^{-\varepsilon}
\]
\begin{proof}
Omitted.\footnote{It's originally intended to be included, but it uses no tool that we've not seen, and you can just sit down and read it if you want.}\\
The constant $C_\varepsilon$ here is \emph{ineffective}: the proof gives no way to calculate $C_\varepsilon$ but just the existence of it (unlike most of the other proofs in this course, where if you really want to calculate the constant you can follow the proof and calculate each step).
\end{proof}
\end{thm}

Proof that theorem 3 $\implies$ theorem 2: if there exists an exceptional zero, then 
\[
\psi(x,\chi) &= O(\frac{x^\beta}{\beta} + x \exp(-c\sqrt{\log x}))\\
&= x O(\exp(-C_\varepsilon q^\varepsilon \log x) + \exp(-C\sqrt{\log x}))
\]
since $q \leq (\log x)^A$, this is $O(\exp -C_\varepsilon' \sqrt{\log x}))$ choosing $\varepsilon = \frac{1}{3A}$, say.


\newpage

\section{Example Class 1}

We'll go through questions 2,3,5,1,4,6 (in that order).

\subsection{Question 2}
(a) 
\[
\sum_{n \leq x} \omega(n) &= \sum_{n \leq x} \sum_{p | n} 1\\
&= \sum_{p \leq x} \sum_{p|n \leq x} 1\\
&= \sum_{p \leq x} \lfloor \frac{x}{p} \rfloor\\
&= x\log\log x + O(x)
\]
using $\sum_{p \leq x} \frac{1}{p} = \log\log x + O(1)$, and $[x] = x+O(1)$ and number of $p\leq x$ is $O(x)$.

Try to avoid writing things like $O(1) \sum_{p\leq x} 1$. Sample replacement of it: $\sum_{p \leq x} O(1) = O(\sum_{p \leq x} 1)$.

(b) In general whenever you seem a sum like this, the first instinct should be to expand it. So we have
\[
&\sum_{n \leq x} |\omega(n) - \log\log x|^2 \ll x\log\log x\\
=&\sum_{n \leq x} \omega(n)^2 - 2\log\log x \sum_{n \leq x} \omega(n) + \lfloor x \rfloor (\log\log x)^2
\]
So it's enough to show that
\[
\sum_{n \leq x} \omega(n)^2 \leq x(\log\log x)^2 + O(x\log \log x)
\]
We write LHS as 
\[
\sum_{p,q \leq x} \sum_{n \leq x} 1_{p|n}1_{q|n} &= \sum_{p=q} \lfloor x/p \rfloor + \sum_{p \neq q} \lfloor \frac{x}{pq}\rfloor\\
&= \sum_{p,q} \lfloor \frac{x}{pq} \rfloor + O(x\log\log x)\\
&\leq x \sum_{p,q} \frac{1}{pq} + O(...)\\
&\leq x (\sum_p \frac{1}{p})^2 + O(...)
\]

(c) It's enough to show that $\sum_{n \leq x} |\log\log x-\log\log n|^2 \ll x\log\log x$ by noting that the sum is actually a metric. We could certainly expand it, but it's more work than what we need. The reason we are expanding above is because we have a mixture of something arithmetic and something analytic, so we can't do anything to it without expanding the square.

Note that if $x^{1/2} \leq n \leq x$, then $|\log\log x - \log\log n| = O(1)$. So the original sum
\[
&= \underbrace{\sum_{n \leq x^{1/2}}}_{O(x^{1/2} (\log\log x)^2} ... + \underbrace{\sum_{x^{1/2}\leq n \leq x}}_{O(x)} ...
\]

Now let $r(x)$ be the number of $n \leq x$ s.t. $|\omega(n) - \log\log n| > (\log \log x)^{3/4}$. Then 
$r(x) (\log\log x)^{3/2} \ll x\log\log x$, so $r(x) \ll \frac{x}{(\log\log x)^{1/2}} = o(x)$. (??)

For 'almost all' $n$, $\tau(n) \geq 2^{\omega(n)}$, and $\leq 2^{\Omega(n)}$ where $\Omega(n)$ counts all prime divisors rather than just the distinct ones.

\subsection{Question 3}

(a) Following lecture, we want something like
\[
\sum_{n \leq x} \frac{1}{n} &= \gamma + \log x - \frac{\{x\}}{x} + \int_x^\infty \frac{\{t\}}{t} dt\\
&= 1 - \frac{\{x\}} + \log x - \int_1^x \frac{\{t\}}{t^2} dt\\
&?= \gamma + \log x - \frac{\{x\}}{x} + \frac{1}{2x} + O(\frac{1}{x^2})
\]

So it's enough to show that 
\[
\left|\int_x^\infty \frac{\{t\}}{t^2} dt - \int_x^\infty \frac{1/2}{t^2} dt\right| \ll \frac{1}{x^2}
\]
Note that for $t \in [n,n+1)$,
\[
\left|\frac{1}{t^2} - \frac{1}{n^2}\right| \ll \frac{1}{n^3}
\]
So
\[
\int_n^{n+1} \frac{\{t\}}{t^2} dt &= \frac{1}{n^2} \int_n^{n+1} \{t\} dt + O(\frac{1}{n^3})\\
&= \frac{1}{2n^2} + O(\frac{1}{n^3})\\
&= \frac{1}{2} \int_n^{n+1} \frac{1}{t^2} dt + O(\frac{1}{n^3})\\
&= \frac{1}{2} \int_n^{n+1} (\frac{1}{t^2} + O(1/t^3)) dt
\]
So
\[
\int_x^\infty \frac{\{t\}}{t^2} dt = \frac{1}{2} \int_x^\infty \frac{1}{t^2} dt + O(\frac{1}{x^2})
\]

(b) (1) Using part (a) we get 
\[
\Delta(x) = x^{1/2} - 2\sum_{a \leq x^{1/2}} \{x/a\} + O(1)
\]
(2) 
\[
\int_0^x \Delta(t) dt \ll x
\]

We get $\frac{2}{3}x^{3/2}$ from the first term after integration, so we want the second term to have a main tern that cancels this out.

\subsection{Question 5}
(a) 
\[
\gamma &= -\int_0^\infty e^{-t} \log t dt\\
&= \lim_{N \to \infty} (\sum_{n=1}^N \frac{1}{n} - \log N)
\]
We write
\[
\frac{1}{n} = \int_I f_n(t) dt = \int_0^1 t^{n-1} dt
\]
Now
\[
\sum_{n=1}^N \frac{1}{n} &= \int_0^1 (1+t+...+t^{N-1}) dt\\
&=\int_0^1 \frac{1-t^N}{1-t} dt\\
&= \int_1^N \frac{1-(1-\frac{v}{N})^N}{v} dv
\]
So
\[
\gamma &= \lim_{n \to \infty} (-\int_1^N \frac{(1-v/N)^N}{v} dv)\\
&= -\int_1^\infty \frac{e^{-v}}{v} dv + \int_0^1 \frac{1-e^{-t}}{t} dt\\
&= \int_1^\infty e^{-t} \log t dt + \int_0^1 e^{-t} \log t dt
\]

(c) $\forall \delta>0$,
\[
\sum_p \frac{1}{p^{1+\delta}} + \log \delta - c + \gamma = \delta \int_2^\infty \frac{E(t)}{t^{1+\delta}} dt - \delta\int_1^2 \frac{\log\log t + c}{t^{1+\delta}}dt
\]
as $\delta \to 0$,
\[
\sum_p \frac{1}{p^{1+\delta}} + \log \delta \to c-\gamma
\]
We have $E(t) \ll \frac{1}{\log t}$, and $\delta \int_2^\infty \frac{1}{(\log t)t^{1+\delta}} dt \ll \delta^{1/2}$.

(e) Note that this will show
\[
\prod_{p \leq x} (1-1/p) \sim e^\gamma \log x
\]
Let 
\[
F(\delta) = \sum_p (\log (1-\frac{1}{p^{1+\delta}})+\frac{1}{p^{1+\delta}})
\]
converges uniformly. So $F(0) = \lim_{\delta \to 0} F(\delta)$.\\
Now 
\[
\sum_p \log (1-\frac{1}{p^{1+\delta}} = \log \zeta(1+\delta) \to -\log \delta
\]
as $\delta \to 0$ (we haven't done zeta function, but let's use it anyway) ($\log \zeta(1+\delta) + \log \delta) \to 0$ as $\delta \to 0$).

\subsection{Question 1}
We want to use induction and hyperbola method as well.\\
Note that $\tau_n = 1*\tau_{n-1}$. Write
\[
\sum_{n\leq x} \tau_k(n) &= \sum_{ab \leq x} \tau_{k-1} (b)\\
&= \sum_{a \leq x^{1/k}} \sum_{b \leq x/a} \tau_{k-1}(b) + \sum_{b \leq x^{1-1/k}} \tau_{k-1}(b) \lfloor x/b\rfloor - (\sum_{b \leq x^{1-1/k}} \tau_{k-1}(b))\lfloor x^{1/k} \rfloor\\
&=...
\]

Unfortunately we have no time for more, but feel free to ask now or later.

（typed solution is online)


\end{document}
